# 301:数据工程

> 原文:[https://winder.ai/301-data-engineering/](https://winder.ai/301-data-engineering/)

**您的工作取决于您的数据**

本节的目标是:

*   谈论什么是数据以及您的领域提供的上下文
*   了解如何处理数据以产生最佳结果
*   了解我们如何以及在哪里可以发现新数据

？？？

如果你没有足够的数据，你将无法在任何数据科学任务中取得成功。

更一般地说，我希望你专注于你的数据。在建立模型之前，有必要了解您的数据。

每种算法都有不同的特征，更适合于特定类型的数据。

* * *

## 您的域中的数据

*   工程师通常不是领域专家
*   你需要

最好的模型是最简单的模型

**提示:领域专家已经在使用模型，找出它们是什么**

？？？

一个经常被忽视的重要方面是领域内的经验。

工程师通常不是领域专家。这影响了他们质疑数据、构建模型和评估结果的能力。

比如预测股市走向 53%的准确率(仅仅比抛硬币好一点)是好是坏？(提示:很好)

当你不是数据领域的专家时，要么学着成为专家，要么确保找到一个专家。

* * *

### 将数据视为一种资产

**数据，以及从数据中提取有价值知识的能力，应被视为战略资产**

？？？

太多时候，数据科学项目是在从现有数据中提取价值的前提下启动的。

当前数据可能不是执行新任务的最佳数据。

始终考虑收集新数据是否更容易和/或更便宜，而不是试图将圆形数据敲进一个方孔。

由数据驱动的公司效率更高；因此更快更有利可图。

对数据、数据科学和工程师的投资会带来回报。

* * *

### 可能是昂贵且耗时的

不要低估收集、解析和理解数据的成本(通常是时间成本)。

？？？

获取数据的成本各不相同。有些数据是免费的，有些数据需要购买。一些数据需要定制的过程来收集。

值得花时间确定收集和使用数据的成本和收益是否值得。

在数据科学中，收集和调查数据是时间的一个重要用途，如果不是最大用途之一的话。因为数据对一个问题来说是如此重要，所以在很低的水平上就能发现巨大的收益。例如添加、改变或删除特征。

* * *

### 必须了解数据

数据一旦收集，就必须了解。

*   你在期待什么？(你的前科)
*   这些数据什么时候可以得到？(数据泄露)
*   它是什么类型的数据？(连续还是分类，随机还是集合？)

**提示:先手动做事。然后您可以验证自动化。**

？？？

你不能指望通过算法运行你不理解的数据并得到结果。

首先，你需要知道你应该期待什么样的结果。只有你能分辨结果是有希望的还是误导的；没有任何指标可以做到这一点(目前还没有！).

此外，有些算法更适合某些类型的数据。

例如，您不会试图通过线性模型推送复杂的分类数据。如果其中一个观察结果明显错误，你应该注意到。

所有这些只有在您理解进入模型的数据的情况下才有可能。

* * *

### 数据收集中的偏差

*   数据有代表性吗？
*   常识真的那么普遍吗？
*   数据是如何记录的？这是否创造了一个子集？

提示:假设你有偏见。假设数据有偏差。

？？？

不要被愚弄，以为数据代表客观事实。

就像“没有 100%这样的事”，也没有代表性数据集这样的事。

一个常见的例子是黑天鹅理论。“黑天鹅”这个词直到 17 世纪还是一个常见的表达，和今天“当猪飞起来”的意思差不多。1697 年，荷兰探险家在澳大利亚发现了一只黑天鹅。

不过，更常见的是，数据收集中会出现偏见。例如，如果我们从银行获得一个数据集，其中包含获得贷款的人的信息，一个常见的问题可能是“我们应该向什么样的人提供贷款”。

但当然，数据是有偏差的。它只包含由以前的人或政策决定的贷款信息。

确保您的数据是整个人口的良好(足够)代表，而不仅仅是一个子集。

* * *

## 预处理数据

*   黑匣子不管用。专家模型可能有用。
*   没有免费的午餐

*预处理数据*的任务是获取原始数据，并尝试将数据与这些假设相匹配，以便算法能够更有效地完成工作。

？？？

一旦收集了数据，我们就不能简单地把数据扔进一个黑匣子，然后期望产生一些有意义的东西。

有一种理论叫做[“没有免费的午餐”](https://en.wikipedia.org/wiki/No_free_lunch_theorem)，它(以高度数学化的方式)声明没有一种技术(算法、优化函数等。)可以在各种数据上表现同样出色。

我们创造模型来简化现实世界。将一组测量数据提炼为更容易理解的形式。在这种简化过程中，所有模型都做出某些假设。

*预处理数据*的任务是获取原始数据，并尝试将数据与这些假设相匹配，以便算法能够更有效地完成工作。

* * *

### 您的数据的特征

*   统计 101
*   所有数据都有一些“特征”
*   我们需要将那个角色形象化，以提高我们对数据的理解
*   直方图是做到这一点的一种方式

*表示*和*标准差*是该特征的量度。

？？？

在我们开始整理数据之前，让我们先来上一堂快速统计学课。

我们采样的每个特征都有一个特征。它以非常独特的方式产生价值。产生非典型的值是不常见的。

如果我们获取特性的值，并将它们分配到条柱，我们可以计算某个值落入条柱的次数。这被称为直方图。

直方图揭示了数据的真实性质，即数据是由一些自然过程形成的。产生这些数据的基础模型称为分布。它完美地描述了这个过程中可能产生的任何新的潜在价值。

* * *

![histogram](../Images/9682ac757b8564324dd61062c406d5d0.png)

* * *

![histogram_fitted](../Images/5c8b4f60902be542d6e6d86a877abccb.png)

* * *

![distributions_1](../Images/9811e6652914471717ea97da3801bcba.png)

* * *

### 坏消息是

**许多算法，从优化器到学习算法，都假设你的数据是正态分布的**

？？？

*   花时间试着使你的数据正常化(变戏法使数据看起来更正常)
*   注意你的数据什么时候不正常，在运行算法的时候记住这一点
*   某些类型的数据可能具有不同的分布，而这种分布是问题所固有的。不要试图将这些数据标准化。
*   绘制特征直方图可让您“感受”数据。即使你不用它来改善数据，它对数据理解也是必不可少的。
*   直方图可能揭示数据中的重要特征。比如有没有数据组？
*   直方图也是可视化问题数据的重要的第一步

* * *

### 缩放比例

几乎所有的算法都希望您的数据具有相同的比例。

*   由于规模被解释为“重要”

(决策树是唯一可以忽略规模的算法之一)

？？？

许多算法(事实上，除了决策树之外的所有算法)都希望数据是某种格式的。其中最主要的是要求所有特征具有相同的比例。

假设我们正在用二维线性模型拟合一些数据。线性算法通过最小化预测结果和数据之间的误差来完成它们的工作。

如果其中一个特征比另一个特征大十倍，那么由预测产生的误差也将是\(10^2\大。因此，试图最小化该误差的优化算法将花费更多的时间来尝试减少较大的误差，因为这是较大的“斜率”。

显然这是错误的，我们不想把任何特性看得比另一个更重要(除非我们这样做，如果我们这样做有更好的方法)。

要解决这个问题，我们需要缩放较大的要素，使其与较小的要素具有相似的特征。

* * *

类:纯表、纯表带区

### 缩放选项

| 名字 | 结果 | 什么时候？ |
| --- | --- | --- |
| 标准鞋匠 | 一的零均值和方差 | 大部分时间 |
| 最小最大标量 | 重新缩放要素，使其介于零和一之间 | 当数据真的为零时，分类值 |
| 鲁棒定标器 | 对异常值不太敏感的 StandardScaler 版本 | 当有异常值时 |

大多数时候我们会使用`StandardScaler`。让我们看一个例子&mldr；

* * *

类别:中间

#### 例子

![scalers](../Images/893277212d3c3add47e34e23872d6b6c.png)

* * *

### 缺少值

如果不是所有的算法，大多数算法都希望数据是数字，并且它们有意义。

*   删除行或列
*   *估算*缺失值

？？？

大多数真实世界的数据并不完美。它缺少值或值无效，如空白、NaNs 或 Nulls。

第一个策略，也是最简单的一个，如果你能负担得起的话，就是丢弃含有无效数据的观察值。

如果您不能丢弃数据，我们可以尝试填充或*估算*这些值；即从数据的已知部分推断它们。

```
>>> import numpy as np
>>> from sklearn.preprocessing import Imputer
>>> imp = Imputer(missing_values='NaN', strategy='mean', axis=0)
>>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])
Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)
>>> X = [[np.nan, 2], [6, np.nan], [7, 6]]
>>> print(imp.transform(X))
[[ 4.          2.        ]
 [ 6.          3.666...]
 [ 7.          6.        ]] 
```

* * *

类:纯表、纯表带区

### 分类特征

如有必要，我们可以将分类特征转换成连续特征:

*   为每个可能的类别创建新列

```
X = [
    {'sex': 'female', 'location': 'Europe', 'age': 33},
    {'sex': 'male', 'location': 'US', 'age': 65},
    {'sex': 'female', 'location': 'Asia', 'age': 48},
] 
```

| 年龄 | 位置=亚洲 | 位置=欧洲 | 位置=美国 | 性别=女性 | 性别=男性 |
| --- | --- | --- | --- | --- | --- |
| Thirty-three | Zero | One | Zero | One | Zero |
| Sixty-five | Zero | Zero | One | Zero | One |
| Forty-eight | One | Zero | Zero | One | Zero |

？？？

当数据以字典的形式呈现时，我们可以使用一个`DictVectorizer`。

```
from sklearn.feature_extraction import DictVectorizer

X = [
    {'sex': 'female', 'location': 'Europe', 'age': 33},
    {'sex': 'male', 'location': 'US', 'age': 65},
    {'sex': 'female', 'location': 'Asia', 'age': 48},
]
vec = DictVectorizer()
X_v = vec.fit_transform(X).toarray()
df = pd.DataFrame(X_v, columns=vec.feature_names_)
print(pandas_df_to_markdown_table(df).data) 
```

| 年龄 | 位置=亚洲 | 位置=欧洲 | 位置=美国 | 性别=女性 | 性别=男性 |
| --- | --- | --- | --- | --- | --- |
| Thirty-three | Zero | One | Zero | One | Zero |
| Sixty-five | Zero | Zero | One | Zero | One |
| Forty-eight | One | Zero | Zero | One | Zero |

参见:`pandas.get_dummies()`执行类似的功能。

有时候，数据不是以字典的形式出现，但仍然是分类的。例如，考虑前面的例子，其中我们使用了一个数值来表示人的不同特征(0 =“女性”，1 =“男性”，等等。)

```
from sklearn.preprocessing import OneHotEncoder

X_v = [[33, 0, 0], [65, 1, 1], [48, 0, 2]]
enc = OneHotEncoder()
X_e = enc.fit_transform(X_v)
print("One hot encoded\n", X_e.toarray())

One hot encoded
 [[ 1.  0.  0.  1.  0.  1.  0.  0.]
 [ 0.  0.  1.  0.  1.  0.  1.  0.]
 [ 0.  1.  0.  1.  0.  0.  0.  1.]] 
```

* * *

类:纯表、纯表带区

### 创建类别

有时添加类别比处理连续变量更有帮助。

*   创建像直方图一样的条柱，并用 1 表示条柱

| 年龄 | 0-10 | 10-20 | 20-30 | 30-40 | 40-50 |
| --- | --- | --- | --- | --- | --- |
| Thirty-three | Zero | One | Zero | Zero | Zero |
| Sixty-five | Zero | Zero | One | Zero | Zero |
| Forty-eight | Zero | Zero | Zero | One | Zero |

* * *

### 保存您的预处理器！

一个小提示:记住跟踪你的预处理器！

因为您已经针对这些预处理过的数据训练了模型，所以模型将只处理已经过预处理的数据。也就是说，您还需要对新数据执行相同的预处理。

* * *