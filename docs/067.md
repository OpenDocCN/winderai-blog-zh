# 202:用于分类的分段

> 原文:[https://winder.ai/202-segmentation-for-classification/](https://winder.ai/202-segmentation-for-classification/)

## 分割

因此，让我们通过一个非常直观的例子来帮助描述所有数据科学算法试图做什么。

如果你以前从未做过这样的事情，这看起来会很复杂。没关系！

我想这样做是为了向你们展示，你们听说过的所有算法，都有一些非常基本的假设，它们试图做什么。

在这之后，我们将会完全地得到一个非常重要的分类器。

* * *

### 问题

假设我们有一个分类问题。我们希望执行模型归纳，以便我们可以生成一个预测模型来决定新啤酒是属于组`A`还是组`B`(二进制分类问题)。

我们有一个指定了目标的数据集，因此这是一个受监督的任务。

解决这个问题的一种方法是根据某种规则任意拆分数据。

例如是`feature 1 > 0.5`

然后我们可以进行测试，看看我们对数据的分割有多好。

我们通过测量结果类的混合程度来做到这一点。

* * *

### 数据

为了更具体地说明这一点，请考虑以下数据:

| Alc。卷 | 颜色 | 班级 |
| --- | --- | --- |
| Four point two | Ninety-two | A |
| Six point four | One hundred and two | A |
| Three point five | three | B |
| Four point seven | Ten | B |

注意:

*   功能
*   不同的音阶
*   上层社会

* * *

### 选择属性

所以我们来思考如何生成一个模型。我们希望将数据分段，这样类就*干净*。

我说的干净，指的是纯粹的，均匀的，清晰的。

如果我们对数据进行切片，最后得到两个类，一个类中全是 A，另一个类中全是 B，那么结果尽可能的纯净。

显然，我们可以通过视觉来做到这一点。我们可以*看到*一个*预测模型*能够解决问题。

但是现实生活并不容易。

* * *

### 熵

我们需要一种方法来衡量一个群体的纯洁性。这被称为*纯度测量*。谢天谢地，我们已经遇到了一个纯度测量。熵。

$$H=-\sum(p_i \log_2 (p_i))$$

其中`\(p_i\)`是观察值属于类别`\(i\)`的概率。

例如，如果我们有两个类:

$ $ H =-P1 \ log _ 2(P1)-p2 \ log _ 2(p2)$ $

* * *

#### 旁白:概率

思想实验:

*   抛钱币
*   扔骰子
*   从一副牌中选一张

你是怎么计算出这个概率的？

有点困难:

*   你下一次见到女人或男人的概率是多少？
*   给定之前的数据(两个 A 和两个 B)，你选择 A 的概率是多少？一个 B 呢？

* * *

考虑前面的例子，我们有两个类，每个类中有两个实例:

$ $ H =-P1 \ log _ 2(P1)-p2 \ log _ 2(p2)$ $

$ $ H =-0.5 \ log _ 2(0.5)-0.5 \ log _ 2(0.5)$ $

$$H=1$$

这是一个非常不纯的数据集。给定观察，该数据具有高熵并且包含最大量的信息。

* * *

熵衡量给定样本中的信息量。

回顾目标，我们希望划分成组，这样当一个新的观察值到达时，我们可以猜测它属于哪个组。

为了选择最佳分割，我们需要衡量哪种分割会产生最佳结果。

我们可以通过尝试不同的分裂来做到这一点，然后再次测量熵，目的是使群体更加纯粹。

我们可以通过计算*信息增益*来测量新的分裂变得有多纯

*注意:在白板上画一张这样的图。*

* * *

#### 信息增益

信息增益被定义为父熵减去子组的总熵。

\begin{align} IG(parent，children)= & entropy(parent)-\ non number \ \
&\ left(p(C1)熵(C1)+p(C2)熵(C2)+&mldr；\右)\end{align}

阅读等式，这是从母熵中减去每个新组的加权熵。

即，我们通过创建两个新类别来计算熵的减少。

* * *

在前面的例子中，父节点的熵是 1 美元。让我们试着用两种方法来划分这些类。

| Alc。卷 | 颜色 | 班级 |
| --- | --- | --- |
| Four point two | Ninety-two | A |
| Six point four | One hundred and two | A |
| Three point five | three | B |
| Four point seven | Ten | B |

* * *

首先让我们除以 5.0 的`Alc. Volume`。这留下了数据:

**< 5.0**

| Alc。卷 | 颜色 | 班级 |
| --- | --- | --- |
| Four point two | Ninety-two | A |
| Three point five | three | B |
| Four point seven | Ten | B |

所以，A = 1/3，B = 2/3

**> = 5.0**

| Alc。卷 | 颜色 | 班级 |
| --- | --- | --- |
| Six point four | One hundred and two | A |

并且，A = 1/1，B = 0/1

* * *

因此，给定父节点(`1.0`)的熵，我们现在可以计算两个子节点的熵:

\ begin { align } H & =-p _ 1 \ log _ 2(p _ 1)-p _ 2 \ log _ 2(p _ 2)\ \
熵(ALC<5.0)&=-0.33 \ log _ 2(0.33)-0.66 \ log _ 2(0.66)\ \
熵(alc < 5.0) & = 0.92 \\
熵(alc > 5.0) & =。

并且计算信息增益为:

\begin{align} IG(parent，children)= & entropy(parent)-\ \
&\ left(p(C1)熵(C1)+p(C2)熵(C2)+&mldr；\ right)\ \
=&1-\ left(\ frac { 3 } { 4 } \乘以 0.92+\ frac { 1 } { 4 } \乘以 0 \ right)\ \
=&0.31 \ \
\ end { align }

通过这种分割，我们获得了`0.31`的信息增益。让我们考虑另一种分裂。

* * *

让我们按 50 的比例分开。这留下了数据:

**<第五十期**

| Alc。卷 | 颜色 | 班级 |
| --- | --- | --- |
| Three point five | three | B |
| Four point seven | Ten | B |

所以，A = 0/2，B = 2/2

【T2 T0】= 50

| Alc。卷 | 颜色 | 班级 |
| --- | --- | --- |
| Four point two | Ninety-two | A |
| Six point four | One hundred and two | A |

并且，A = 2/2，B = 0/2

* * *

\ begin { align } H & =-p _ 1 \ log _ 2(p _ 1)-p _ 2 \ log _ 2(p _ 2)\ \
熵(ALC<5.0)&=-1 \ log _ 2(1)-0 \ \
熵(alc < 5.0) & = 0 \\
熵(ALC>5.0)&=-1 \ log _ 2(1)\ \
熵(alc > 5 .

并且计算信息增益为:

\begin{align} IG(parent，children)= & entropy(parent)-\ \
&\ left(p(C1)熵(C1)+p(C2)熵(C2)+&mldr；\ right)\ \
=&1-\ left(\ frac { 2 } { 4 } \ times 0+\ frac { 2 } { 4 } \ times 0 \ right)\ \
=&1 \ end { align }

一个`1`的信息增益。对比两次拆分，第一次只增加了`0.31`的信息量。这种分裂使 IG 增加了整整一个`1.0`。显然，我们会选择第二次拆分来拆分数据。

* * *