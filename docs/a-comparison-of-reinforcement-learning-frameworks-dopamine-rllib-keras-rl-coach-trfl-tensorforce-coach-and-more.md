# å¼ºåŒ–å­¦ä¹ æ¡†æ¶çš„æ¯”è¾ƒ:å¤šå·´èƒºã€RLLibã€Keras-RLã€è”»é©°ã€TRFLã€Tensorforceã€è”»é©°ç­‰ç­‰

> åŸæ–‡ï¼š<https://winder.ai/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/>

*å¼ºåŒ–å­¦ä¹ * (RL)æ¡†æ¶é€šè¿‡åˆ›å»º RL ç®—æ³•æ ¸å¿ƒç»„ä»¶çš„æ›´é«˜çº§æŠ½è±¡æ¥å¸®åŠ©å·¥ç¨‹å¸ˆã€‚è¿™ä½¿å¾—ä»£ç æ›´å®¹æ˜“å¼€å‘ï¼Œæ›´å®¹æ˜“é˜…è¯»ï¼Œå¹¶æé«˜æ•ˆç‡ã€‚

ä½†æ˜¯é€‰æ‹©ä¸€ä¸ªæ¡†æ¶ä¼šå¼•å…¥ä¸€äº›é™åˆ¶ã€‚åœ¨å­¦ä¹ å’Œä½¿ç”¨ä¸€ä¸ªæ¡†æ¶ä¸Šçš„æŠ•èµ„ä¼šä½¿å®ƒå¾ˆéš¾è„±ç¦»ã€‚è¿™å°±åƒä½ å†³å®šå»å“ªå®¶é…’å§ä¸€æ ·ã€‚ä¸ç®¡è¿™ä¸ªåœ°æ–¹æœ‰å¤šç³Ÿç³•ï¼Œéƒ½å¾ˆéš¾ä¸ä¹°å•¤é…’ã€‚

 [æˆ‘å…³äºå¼ºåŒ–å­¦ä¹ çš„æ–°ä¹¦](https://rl-book.com/?utm_source=winderresearch&utm_medium=web&utm_campaign=rl) 

ä½ æƒ³åœ¨ç°å®ç”Ÿæ´»ã€å•†ä¸šåº”ç”¨ä¸­ä½¿ç”¨ RL å—ï¼Ÿä½ æƒ³çŸ¥é“çœŸç›¸å—ï¼Ÿæœ€ä½³å®è·µï¼Ÿ

æˆ‘ä»¬ä¸º O'Reilly å†™äº†ä¸€æœ¬å…³äºå¼ºåŒ–å­¦ä¹ çš„ä¹¦ã€‚å®ƒä¾§é‡äºå·¥ä¸š RLï¼Œæœ‰è®¸å¤šçœŸå®ç”Ÿæ´»çš„ä¾‹å­å’Œæ·±å…¥çš„åˆ†æã€‚

äº†è§£æ›´å¤šå…³äº https://rl-book.com çš„ä¿¡æ¯ã€‚

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘æä¾›äº†ä¸€äº›å…³äºæœ€æµè¡Œçš„ RL æ¡†æ¶çš„æ³¨é‡Šã€‚æˆ‘è¿˜å±•ç¤ºäº† Github å’Œ Google(ä½ ä¸èƒ½ç›¸ä¿¡å®ƒä»¬)çš„ä¸€äº›ç²—ç•¥çš„ç»Ÿè®¡æ•°æ®ï¼Œè¯•å›¾é‡åŒ–å®ƒä»¬çš„å—æ¬¢è¿ç¨‹åº¦ã€‚

This post is now out of date. Please double check that what I discuss here is still relevant and/or correct.

## æœ¬ä½œå“çš„æœ€åˆç›®çš„

æˆ‘æ­£åœ¨ä¸ºå¥¥èµ–åˆ©å†™ä¸€æœ¬å…³äº RL çš„ä¹¦ã€‚ä½œä¸ºè¯¥ä¹¦çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘æƒ³å‘æˆ‘çš„è¯»è€…å±•ç¤ºå¦‚ä½•æ„å»ºå’Œè®¾è®¡å„ç§ RL ä»£ç†ã€‚æˆ‘è®¤ä¸ºè¯»è€…ä¼šä»ä½¿ç”¨å·²ç»å»ºç«‹çš„æ¡†æ¶æˆ–åº“çš„ä»£ç ä¸­å—ç›Šã€‚æ— è®ºå¦‚ä½•ï¼Œç¼–å†™è¿™äº›æ¡†æ¶çš„äººå¯èƒ½ä¼šæ¯”æˆ‘åšå¾—æ›´å¥½ã€‚

æ‰€ä»¥é—®é¢˜æ˜¯ï¼Œâ€œå“ªä¸ªæ¡†æ¶ï¼Ÿâ€ã€‚å¼•å¯¼æˆ‘èµ°ä¸Šè¿™æ¡è·¯ã€‚ä¸€å¼€å§‹åªæœ‰å‡ ä¸ªæ¡†æ¶ï¼Œä½†åæ¥æˆ‘å‘ç°äº†æ›´å¤šã€‚è¿˜æœ‰æ›´å¤šã€‚äº‹å®è¯æ˜ï¼Œå·²ç»æœ‰ç›¸å½“å¤šçš„æ¡†æ¶å¯ç”¨ï¼Œè¿™å°±å˜æˆäº†ä¸€ä¸ª 8000 å­—çš„åºç„¶å¤§ç‰©ã€‚æå‰ä¸ºç¯‡å¹…é“æ­‰ã€‚æˆ‘ä¸æŒ‡æœ›æœ‰å¤šå°‘äººèƒ½å…¨éƒ¨è¯»å®Œï¼

å› ä¸ºç¯‡å¹…çš„åŸå› ï¼Œè¿™ä¹ŸèŠ±äº†ä¸€æ®µæ—¶é—´æ¥å†™ã€‚è¿™æ„å‘³ç€è¯„è®ºæ²¡æœ‰æ¿€å…‰ç„¦ç‚¹ã€‚æœ‰æ—¶æˆ‘åœ¨ä¸€ä¸ªæ¡†æ¶ä¸­è¯„è®ºä¸€ä»¶äº‹ï¼Œè€Œåœ¨å¦ä¸€ä¸ªæ¡†æ¶ä¸­æ ¹æœ¬ä¸è¯„è®ºã€‚ä¸ºæ­¤é“æ­‰ï¼›å®ƒå¹¶ä¸æ˜¯è¯¦å°½æ— é—çš„ã€‚

## æ–¹æ³•å­¦

å¤§éƒ¨åˆ†è¯„ä»·éƒ½æ˜¯çº¯è§‚ç‚¹ã€‚ä½†æ˜¯æˆ‘ä»¬å¯ä»¥çœ‹ä¸€äº›é‡åŒ–çš„æŒ‡æ ‡ã€‚å³åœ¨ Github ä¸­å¯ä»¥è·å¾—çš„å­˜å‚¨åº“çš„ç»Ÿè®¡æ•°æ®ã€‚èµ·ç‚¹å¤§è‡´ä»£è¡¨äº†æ¯ä¸ªæ¡†æ¶çš„çŸ¥ååº¦ï¼Œä½†å¹¶ä¸ä»£è¡¨è´¨é‡ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œæ‹¥æœ‰æœ€å¤šæ˜æ˜Ÿçš„æ¡†æ¶æ‹¥æœ‰æ›´å¼ºçš„è¥é”€èƒ½åŠ›ã€‚

ä¹‹åï¼Œæˆ‘ä¸€ç›´åœ¨å¯»æ‰¾æ¨¡å—åŒ–ã€æ˜“ç”¨æ€§ã€çµæ´»æ€§å’Œæˆç†Ÿæ€§çš„ç»“åˆã€‚ç®€å•æ€§ä¹Ÿæ˜¯éœ€è¦çš„ï¼Œä½†æ˜¯è¿™é€šå¸¸ä¸æ¨¡å—åŒ–å’Œçµæ´»æ€§æ˜¯ç›¸äº’æ’æ–¥çš„ã€‚ä¸‹é¢æå‡ºçš„æ„è§æ˜¯åŸºäºè¿™äº›ç†æƒ³ã€‚

ä¸€ä¸ªåå¤å‡ºç°çš„ä¸»é¢˜æ˜¯ RL æ¡†æ¶ä¸­*æ·±åº¦å­¦ä¹ * (DL)æ¡†æ¶çš„ä¸»å¯¼åœ°ä½ã€‚DL æ¡†æ¶é€šå¸¸ä¼šçªç ´æŠ½è±¡ï¼Œè€Œ RL æ¡†æ¶åªæ˜¯å‰è€…çš„æ‰©å±•ã€‚è¿™æ„å‘³ç€ï¼Œå¦‚æœæ‚¨å·²ç»æœ‰äº†ä¸€ä¸ªç‰¹å®šçš„ DL è§£å†³æ–¹æ¡ˆï¼Œé‚£ä¹ˆæ‚¨è¿˜ä¸å¦‚åšæŒä¸‹å»ã€‚

ä½†å¯¹æˆ‘æ¥è¯´ï¼Œè¿™ä»£è¡¨ç€é”å®šã€‚æˆ‘çš„åå¥½æ€»æ˜¯å€¾å‘äºä¸å¼ºåˆ¶è¦æ±‚ç‰¹å®šçš„ DL å®ç°æˆ–è€…æ ¹æœ¬ä¸ä½¿ç”¨ DL çš„æ¡†æ¶(éœ‡æƒŠ/ææ€–ï¼).ç»“æœæ˜¯ï¼Œæ‰€æœ‰çš„ Google æ¡†æ¶éƒ½å€¾å‘äº Tensorflowï¼Œæ‰€æœ‰çš„å­¦æœ¯æ¡†æ¶éƒ½ä½¿ç”¨ PyTorchï¼Œç„¶åæœ‰ä¸€äº›å‹‡æ•¢çš„äººå¾˜å¾Šåœ¨ä¸­é—´ï¼Œä»–ä»¬çš„å·¥ä½œé‡æ˜¯å…¶ä»–äººçš„ä¸¤å€ã€‚

æˆ‘è¿˜è¯•å›¾æŸ¥çœ‹æ¯ä¸ªæ¡†æ¶çš„è°·æ­Œæ’åï¼Œä½†ç»“æœè¯æ˜[å¹¶ä¸å¯é ](#google-rankings)ã€‚

## éšé™„ç¬”è®°æœ¬

åœ¨å¯èƒ½çš„æƒ…å†µä¸‹ï¼Œæˆ–è€…æœ‰æ„ä¹‰çš„æƒ…å†µä¸‹ï¼Œæˆ‘å°è¯•äº†å¾ˆå¤šè¿™æ ·çš„æ¡†æ¶ã€‚ä»–ä»¬ä¸­çš„è®¸å¤šäººæ²¡æœ‰å·¥ä½œã€‚ä»–ä»¬ä¸­çš„ä¸€äº›äººä¸€å¼€å§‹å°±æœ‰å¾ˆæ£’çš„ç¬”è®°æœ¬ï¼Œæ‰€ä»¥ä½ å¯ä»¥å»çœ‹çœ‹ã€‚

è‡³äºå…¶ä»–çš„ï¼Œæˆ‘å·²ç»[å‘è¡¨äº†ä¸€ä¸ªè¦ç‚¹ï¼Œä½ å¯ä»¥åœ¨è°·æ­Œå®éªŒå®¤](https://colab.research.google.com/gist/philwinder/07cbe7b696745ac25c0f6a2aadbcd3c7/framework-research.ipynb)ä¸Šè¿è¡Œã€‚è¿™æ˜¯ä»¥éå¸¸åŸå§‹çš„æ ¼å¼å‘ˆç°çš„ã€‚å®ƒå¹¶ä¸æ„å‘³ç€æ˜¯å…¨é¢çš„æˆ–è§£é‡Šæ€§çš„ã€‚æˆ‘åªæ˜¯æƒ³ä»”ç»†æ£€æŸ¥ä¸€ä¸‹ï¼Œåœ¨æœ€ç®€å•çš„æƒ…å†µä¸‹ï¼Œå®ƒæ˜¯å¦å·¥ä½œã€‚

åœ¨æ¯ä¸€èŠ‚ä¸­ï¼Œæˆ‘è¿˜å±•ç¤ºäº†ä¸€ä¸ªâ€œå…¥é—¨â€å°æ ‡é¢˜ï¼Œå±•ç¤ºäº†æ¯ä¸ªæ¡†æ¶çš„åŸºæœ¬ç¤ºä¾‹ã€‚è¿™æ˜¯ç¬”è®°æœ¬ä¸Šçš„ä»£ç ã€‚

## å¼ºåŒ–å­¦ä¹ æ¡†æ¶

ä»¥ä¸‹æ¡†æ¶æ˜¯æŒ‰ç…§æˆªè‡³ 2019 å¹´ 6 æœˆå…¶ Github çŸ¥è¯†åº“ä¸­çš„æ˜Ÿçº§æ•°æ’åˆ—çš„ã€‚æ˜Ÿå·çš„å®é™…æ•°é‡å’Œå…¶ä»–æŒ‡æ ‡ä»¥å¾½ç« çš„å½¢å¼æ˜¾ç¤ºåœ¨æ¯ä¸ªæ¡†æ¶çš„æ ‡é¢˜ä¸‹æ–¹ã€‚

æ¯”è¾ƒäº†ä»¥ä¸‹æ¡†æ¶:

*   [OpenAI å¥èº«æˆ¿](#openai-gym-https-github-com-openai-gym)
*   [è°·æ­Œå¤šå·´èƒº](#google-dopamine-https-github-com-google-dopamine)
*   [RLLib](#rllib-https-ray-readthedocs-io-en-latest-rllib-html-via-ray-project-https-github-com-ray-project-ray)
*   [Keras-RL](#keras-rl-https-github-com-keras-rl-keras-rl)
*   [TRFL](#trfl-https-github-com-deepmind-trfl)
*   [å¼ é‡åŠ›](#tensorforce-https-github-com-tensorforce-tensorforce)
*   [è„¸ä¹¦åœ°å¹³çº¿](#horizon-https-github-com-facebookresearch-horizon)
*   è”»é©°ç¥ç»ç³»ç»Ÿå…¬å¸
*   [MAgent](#magent-https-github-com-geek-ai-magent)
*   [SLM-Lab](#slm-lab-https-github-com-kengz-slm-lab)
*   [é¹¿](#deer-https-github-com-vinf-deer)
*   [è½¦åº“](#garage-https-github-com-rlworkgroup-garage)
*   [è¶…ç°å®](#surreal-https-github-com-surrealai-surreal)
*   [RLgraph](#rlgraph-https-github-com-rlgraph-rlgraph)
*   [ç®€å• RL](#simple-rl-https-github-com-david-abel-simple-rl)

### [OpenAI å¥èº«æˆ¿](https://github.com/openai/gym)

![](img/1f7dd55221b32e41c099b499171ed2a5.png)![](img/44c932c94d033a3fe96e6941a7007dde.png)![](img/d4e35f3e394c660f4762617ee0780072.png)![](img/6e2d01a6713540bb82546671b4462db9.png)

OpenAI æ˜¯ä¸€å®¶éè¥åˆ©æ€§çš„çº¯ç ”ç©¶å…¬å¸ã€‚æä¾›ä¸€ç³»åˆ—å¼€æºçš„æ·±åº¦å’Œå¼ºåŒ–å­¦ä¹ å·¥å…·ï¼Œä»¥æé«˜å¯é‡å¤æ€§ã€åˆ›å»ºåŸºå‡†å¹¶æ”¹è¿›æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚æˆ‘å–œæ¬¢å°†å®ƒä»¬è§†ä¸ºå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œä¹‹é—´çš„æ¡¥æ¢ã€‚

ä½†æˆ‘çŸ¥é“ä½ åœ¨æƒ³ä»€ä¹ˆã€‚â€œè²å°”ï¼Œå¥èº«æˆ¿ä¸æ˜¯ä¸€ä¸ªæ¡†æ¶ã€‚æ˜¯ä¸€ä¸ªç¯å¢ƒã€‚â€ã€‚æˆ‘çŸ¥é“ï¼Œæˆ‘çŸ¥é“ã€‚å®ƒæä¾›äº†ä¸€ç³»åˆ—çš„ç©å…·ç¯å¢ƒï¼Œç»å…¸æ§åˆ¶ï¼Œæœºå™¨äººï¼Œè§†é¢‘æ¸¸æˆå’Œæ£‹ç›˜æ¸¸æˆæ¥æµ‹è¯•ä½ çš„ RL ç®—æ³•ã€‚

ä½†æ˜¯æˆ‘æŠŠå®ƒåŒ…æ‹¬åœ¨è¿™é‡Œï¼Œå› ä¸ºå®ƒç»å¸¸è¢«ç”¨ä½œå®šåˆ¶å·¥ä½œçš„åŸºç¡€ã€‚äººä»¬åƒä½¿ç”¨æ¡†æ¶ä¸€æ ·ä½¿ç”¨å®ƒã€‚å¯ä»¥æŠŠå®ƒçœ‹ä½œæ˜¯ RL å®ç°å’Œç¯å¢ƒä¹‹é—´çš„æ¥å£ã€‚å®ƒéå¸¸ä¸°å¯Œï¼Œä¸‹é¢åˆ—å‡ºçš„è®¸å¤šå…¶ä»–æ¡†æ¶ä¹Ÿä¸ Gym æ¥å£ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥ä½œä¸ºæ¯”è¾ƒä¸€åˆ‡çš„åŸºå‡†ã€‚å› ä¸ºè¿™æ˜¯ RL ä¸­æœ€æµè¡Œçš„åº“ä¹‹ä¸€ã€‚

#### å…¥é—¨æŒ‡å—

å¥èº«æˆ¿æ—¢é…·åˆæœ‰é—®é¢˜ï¼Œå› ä¸ºå®ƒçš„ç°å® 3D ç¯å¢ƒã€‚å¦‚æœä½ æƒ³å¯è§†åŒ–æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ï¼Œä½ éœ€è¦èƒ½å¤Ÿæ¸²æŸ“è¿™äº›ç¯å¢ƒã€‚å®ƒå‡ ä¹å¯ä»¥åœ¨ä½ çš„ç¬”è®°æœ¬ç”µè„‘ä¸Šè¿è¡Œï¼Œä½†æ˜¯å½“ä½ è¯•å›¾åœ¨ç¬”è®°æœ¬ç”µè„‘ä¸Šè¿è¡Œå®ƒæ—¶ï¼Œç”±äºæµè§ˆå™¨çš„é™åˆ¶ï¼Œå®ƒä¼šå¾ˆåƒåŠ›ã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½ å¿…é¡»ä½¿ç”¨è™šæ‹Ÿæ˜¾ç¤ºå™¨ã€‚åŸºæœ¬ä¸Šï¼Œæˆ‘ä»¬å¿…é¡»æ¨¡æ‹Ÿè§†é¢‘é©±åŠ¨ç¨‹åºã€‚è¿™æ„å‘³ç€å¤§å¤šæ•°â€œå…¥é—¨â€ä»£ç æ˜¯è§†é¢‘åŒ…è£…ä»£ç ã€‚

å¦‚æœæˆ‘ä»¬å¿½ç•¥æ‰€æœ‰æ— èŠçš„ä¸œè¥¿ï¼Œä½ å¯ä»¥åœ¨é™„å¸¦çš„ç¬”è®°æœ¬ä¸­æ‰¾åˆ°ï¼Œæ ¸å¿ƒå¥èº«æˆ¿ä»£ç çœ‹èµ·æ¥åƒ:

```py
import gym
from gym.wrappers.monitoring.video_recorder import VideoRecorder # Because we want to record a video

env = gym.make("CartPole-v1") # Create the cartpole environment
rec = VideoRecorder(env)      # Create the video recorder
rec.capture_frame()           # Capture the starting position
while True:
    action = env.action_space.sample()                   # Use a random action
    observation, reward, done, info = env.step(action)   # to take a single step in the environment
    rec.capture_frame()                                  # and record
    if done:
           break                                         # If the pole has fallen, quit.
rec.close()  # Close the recording
env.close()  # Close the environment 
```

å¦‚ä½ æ‰€è§ï¼Œæˆ‘ä»¬ä¸€ç›´åœ¨è¡ŒåŠ¨ï¼Œç›´åˆ°æ†å­å€’ä¸‹ã€‚è¿™ä¸ªç®€å•çš„ API å¯ä»¥åœ¨æ‰€æœ‰ç¯å¢ƒä¸­é‡å¤ä½¿ç”¨ã€‚è¿™å˜å¾—å¦‚æ­¤æµè¡Œï¼Œä»¥è‡³äºäººä»¬å·²ç»æ„å»ºäº†ä½¿ç”¨ç›¸åŒ API ä½†å…·æœ‰æ–°ç¯å¢ƒçš„æ‰©å±•ã€‚

[https://winder.ai/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/gym.mp4](https://winder.ai/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/gym.mp4)

æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œå®ƒç›´æ¥ä¸‹é™äº†ï¼Œå› ä¸ºæ­¤åˆ»æˆ‘ä»¬åªæ˜¯é€šè¿‡éšæœºåŠ¨ä½œã€‚ä½†æ˜¯ï¼Œä»ç„¶æœ‰ä¸€äº›å‚¬çœ çš„ä¸œè¥¿ï¼Œä¸€äº›é¼“å’Œä½éŸ³ã€‚

ä½†æ˜¯ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹ä¸€äº›ä»…ä»£ç†æ¡†æ¶é€‰é¡¹ã€‚

### [è°·æ­Œå¤šå·´èƒº](https://github.com/google/dopamine)

![](img/87a503a4f522ceb3b6887edbad9ca753.png)![](img/1b5a17702fac31d891b5a31e26a8ddf8.png)![](img/4c5c6b53f57e0acd749761fd05893e17.png)![](img/2757d88bbb200d6cb8c9cf7b793efb5c.png)

è°·æ­Œå¤šå·´èƒº:â€œä¸æ˜¯è°·æ­Œçš„å®˜æ–¹äº§å“â€(NOGPâ€”â€”æˆ‘ç°åœ¨è¦åˆ›é€ çš„é¦–å­—æ¯ç¼©å†™è¯)ï¼Œä½†ç”±è°·æ­Œå‘˜å·¥ç¼–å†™ï¼Œæ‰˜ç®¡åœ¨è°·æ­Œ github ä¸Šã€‚é‚£å°±è°·æ­Œä¸€ä¸‹å¤šå·´èƒºã€‚å®ƒæ˜¯ RL æ¡†æ¶é¢†åŸŸçš„ä¸€ä¸ªç›¸å¯¹è¾ƒæ–°çš„è¿›å…¥è€…ï¼Œçœ‹èµ·æ¥å¾ˆå—æ¬¢è¿ã€‚å®ƒæ‹¥æœ‰å¤§é‡çš„ Github æ˜æ˜Ÿå’Œä¸€äº›è°·æ­Œè¶‹åŠ¿æ’åã€‚è¿™å°¤å…¶ä»¤äººæƒŠè®¶ï¼Œå› ä¸ºè‡ªé¡¹ç›®å¯åŠ¨ä»¥æ¥ï¼Œæäº¤çš„æ•°é‡ã€æäº¤è€…å’Œæ—¶é—´éƒ½æ˜¯æœ‰é™çš„ã€‚æ˜¾ç„¶ï¼Œè°·æ­Œçš„å“ç‰Œå’Œè¥é”€éƒ¨é—¨å¯¹ä½ æœ‰å¸®åŠ©ã€‚

æ— è®ºå¦‚ä½•ï¼Œè¿™ä¸ªæ¡†æ¶æœ€é…·çš„ä¸€ç‚¹æ˜¯ï¼Œå®ƒé€šè¿‡ä½¿ç”¨ [Google gin-config](https://github.com/google/gin-config) é…ç½®æ¡†æ¶ï¼Œå¼ºè°ƒäº†ä½œä¸ºä»£ç çš„é…ç½®ã€‚è¿™ä¸ªæƒ³æ³•æ˜¯ä½ æœ‰è®¸å¤šå¯æ’å…¥çš„ä½ï¼Œä½ é€šè¿‡ä¸€ä¸ªé…ç½®æ–‡ä»¶è¿æ¥åœ¨ä¸€èµ·ã€‚å¥½å¤„æ˜¯è¿™å…è®¸äººä»¬å‘å¸ƒä¸€ä¸ªå•ä¸€çš„é…ç½®æ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶åŒ…å«ç‰¹å®šäºè¯¥è¿è¡Œçš„æ‰€æœ‰å‚æ•°ã€‚gin-config è®©äº‹æƒ…å˜å¾—ç‰¹åˆ«ï¼Œå› ä¸ºå®ƒå…è®¸æ‚¨å°†å¯¹è±¡è¿æ¥åœ¨ä¸€èµ·ï¼›ç±»å’Œ lambdas ä¹‹ç±»çš„å®ä¾‹ã€‚

ä¸åˆ©çš„ä¸€é¢æ˜¯ä½ å¢åŠ äº†é…ç½®æ–‡ä»¶çš„å¤æ‚æ€§ï¼Œå®ƒæœ€ç»ˆä¼šåƒå¦ä¸€ä¸ªå……æ»¡ä»£ç çš„æ–‡ä»¶ä¸€æ ·ï¼Œäººä»¬æ— æ³•ç†è§£ï¼Œå› ä¸ºä»–ä»¬ä¸ä¹ æƒ¯ã€‚ä¸¾ä¾‹æ¥è¯´ï¼Œå°±æˆ‘ä¸ªäººè€Œè¨€ï¼Œæˆ‘æ€»æ˜¯åšæŒä½¿ç”¨â€œæ„šè ¢â€çš„é…ç½®æ–‡ä»¶ï¼Œæ¯”å¦‚ Kubernetes Manifests æˆ– JSON(å°±åƒè®¸å¤šå…¶ä»–æ¡†æ¶ä¸€æ ·)ã€‚å¸ƒçº¿åº”è¯¥æŒ‰ç…§ä»£ç è¿›è¡Œã€‚

ä¸€ä¸ªä¸»è¦çš„å¥½å¤„æ˜¯å®ƒä¿ƒè¿›äº†å¯æ’æ‹”æ€§å’Œå¯é‡ç”¨æ€§ï¼Œè¿™æ˜¯åœ¨å¼€å‘[æ•°æ®ç§‘å­¦](https://winder.ai/what-is-data-science/)äº§å“æ—¶ç»å¸¸è¢«å¿½ç•¥çš„å…³é”® OOP å’ŒåŠŸèƒ½æ¦‚å¿µã€‚

æ˜¾ç„¶ï¼Œå®ƒåœ¨å¾ˆçŸ­çš„æ—¶é—´å†…è·å¾—äº†å¾ˆå¤§çš„å¸å¼•åŠ›ã€‚å¦ç™½è¯´ï¼Œè¿™è®©æˆ‘æœ‰ç‚¹æ‹…å¿ƒã€‚æœ‰å››ä¸ªè´¡çŒ®è€…ï¼Œåªæœ‰ 100 ä¸ªæäº¤ã€‚åœ¨è¿™å››ä¸ªäººä¸­ï¼Œä¸‰ä¸ªæ¥è‡ªç¤¾åŒº(bug-fixesï¼Œç­‰ç­‰)ã€‚).è¿™å°±å‰©ä¸‹ä¸€ä¸ªäººäº†ã€‚è€Œè¿™ä¸€ä¸ªäººå·²ç»çŠ¯äº†ï¼Œç­‰ç€å§ï¼Œ [**è¶…è¿‡ 130 ä¸‡è¡Œä»£ç **](https://github.com/google/dopamine/graphs/contributors) ã€‚

å¾ˆæ˜æ˜¾è¿™é‡Œæœ‰çŒ«è…»ã€‚ä»æäº¤å†å²æ¥çœ‹ï¼Œä»£ç ä¼¼ä¹æ˜¯ä»å¦ä¸€ä¸ª repo è½¬ç§»è¿‡æ¥çš„[ã€‚120 ä¸‡è¡Œæäº¤å¹¶ä¸æ˜¯æœ€ä½³å®è·µï¼:-)è¿™æ˜¯ Apache è®¸å¯çš„ï¼Œæ‰€ä»¥æ²¡æœ‰ä»€ä¹ˆå¤ªå¥‡æ€ªçš„äº‹æƒ…å‘ç”Ÿï¼Œä½†ç‰ˆæƒå·²è¢«åˆ†é…ç»™](https://github.com/google/dopamine/commit/420b147474d455fe39d911432fe579e54db3a1e0)[è°·æ­Œå…¬å¸](https://github.com/google/dopamine)ã€‚ä½†æ˜¯æˆ‘å¯¹[è´¡çŒ®è€…åè®®](https://github.com/google/dopamine/blob/master/CONTRIBUTING.md#contributor-license-agreement)æ„Ÿåˆ°æ”¾å¿ƒã€‚

å°±æ¨¡å—åŒ–è€Œè¨€ï¼Œå¹¶ä¸å¤šã€‚å¯¹äº[ä»£ç†](https://github.com/google/dopamine/tree/master/dopamine/agents)æ²¡æœ‰ä»»ä½•æŠ½è±¡ï¼›å®ƒä»¬æ˜¯ç›´æ¥å®ç°çš„ï¼Œå¹¶ä» gin é…ç½®ä¸­è¿›è¡Œé…ç½®ã€‚å®ç°çš„ä¹Ÿä¸å¤šã€‚ä¹Ÿæ²¡æœ‰ä»»ä½•å®˜æ–¹çš„ç¯å¢ƒæŠ½è±¡ã€‚äº‹å®ä¸Šï¼Œçœ‹èµ·æ¥å®ƒä»¬åªæ˜¯åˆ°å¤„ä¼ å›æ ¸å¿ƒ Tensorflow å¯¹è±¡ï¼Œå¹¶å‡è®¾ä½¿ç”¨ Tensorflow æ¥å£ã€‚ç®€è€Œè¨€ä¹‹ï¼Œéå¸¸å°‘çš„å®˜æ–¹ OOP é£æ ¼çš„æŠ½è±¡ï¼Œè¿™ä¸åŒäºå¤§å¤šæ•°å…¶ä»–æ¡†æ¶ã€‚

ç®€è€Œè¨€ä¹‹ï¼Œå°æ¨¡å—åŒ–ï¼Œé‡ç”¨æ˜¯ç¬¨æ‹™çš„(IMO ),å°½ç®¡å®ƒä¼¼ä¹å¾ˆå—æ¬¢è¿ï¼Œä½†å®ƒä¸æ˜¯å¾ˆæˆç†Ÿï¼Œä¹Ÿæ²¡æœ‰ç¤¾åŒºæ”¯æŒã€‚

#### å…¥é—¨æŒ‡å—

åŒæ ·ï¼Œæ‚¨å¯ä»¥åœ¨éšé™„çš„ç¬”è®°æœ¬ä¸­æ‰¾åˆ°ç¤ºä¾‹ï¼Œä½†å‰ææ˜¯é€šè¿‡é…ç½®æ–‡ä»¶æ„å»ºæ‚¨çš„ RL ç®—æ³•ã€‚çœ‹èµ·æ¥æ˜¯è¿™æ ·çš„:

```py
DQN_PATH = os.path.join(BASE_PATH, 'dqn')
# Modified from dopamine/agents/dqn/config/dqn_cartpole.gin
dqn_config = """ # Hyperparameters for a simple DQN-style Cartpole agent. The hyperparameters # chosen achieve reasonable performance. import dopamine.discrete_domains.gym_lib import dopamine.discrete_domains.run_experiment import dopamine.agents.dqn.dqn_agent import dopamine.replay_memory.circular_replay_buffer import gin.tf.external_configurables   DQNAgent.observation_shape = %gym_lib.CARTPOLE_OBSERVATION_SHAPE DQNAgent.observation_dtype = %gym_lib.CARTPOLE_OBSERVATION_DTYPE DQNAgent.stack_size = %gym_lib.CARTPOLE_STACK_SIZE DQNAgent.network = @gym_lib.cartpole_dqn_network DQNAgent.gamma = 0.99 DQNAgent.update_horizon = 1 DQNAgent.min_replay_history = 500 DQNAgent.update_period = 4 DQNAgent.target_update_period = 100 DQNAgent.epsilon_fn = @dqn_agent.identity_epsilon DQNAgent.tf_device = '/gpu:0'  # use '/cpu:*' for non-GPU version DQNAgent.optimizer = @tf.train.AdamOptimizer()   tf.train.AdamOptimizer.learning_rate = 0.001 tf.train.AdamOptimizer.epsilon = 0.0003125   create_gym_environment.environment_name = 'CartPole' create_gym_environment.version = 'v0' create_agent.agent_name = 'dqn' TrainRunner.create_environment_fn = @gym_lib.create_gym_environment Runner.num_iterations = 100 Runner.training_steps = 100 Runner.evaluation_steps = 100 Runner.max_steps_per_episode = 200  # Default max episode length.   WrappedReplayBuffer.replay_capacity = 50000 WrappedReplayBuffer.batch_size = 128 """
gin.parse_config(dqn_config, skip_unknown=False) 
```

é‚£å·²ç»å¾ˆå¤šäº†ã€‚ä½†æ˜¯å®ƒå®ç°äº†ä¸€ä¸ªæ›´å¤æ‚çš„ç®—æ³•ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥é¢„æ–™åˆ°ã€‚æˆ‘å¾ˆé«˜å…´é‚£é‡Œæœ‰è¶…å‚æ•°ï¼Œä½†æ˜¯æˆ‘ä¸ç¡®å®šæˆ‘æ˜¯ä¸æ˜¯æ‰€æœ‰åŠ¨æ€æ³¨å…¥çš„ç²‰ä¸(`@`è¡¨ç¤ºä¸€ä¸ªç±»çš„å®ä¾‹)ã€‚æ”¯æŒè€…ä¼šè¯´â€œå“‡ï¼Œçœ‹ï¼Œæˆ‘åªè¦æ”¹å˜è¿™æ¡çº¿å°±å¯ä»¥æŠŠä¹è§‚è€…æ¢å‡ºæ¥â€ã€‚ä½†æ˜¯æˆ‘è®¤ä¸ºæˆ‘ä¹Ÿå¯ä»¥ç”¨æ™®é€šçš„è€ Python æ¥åšè¿™ä»¶äº‹ã€‚

ç»è¿‡ä¸€ç‚¹è®­ç»ƒå:

```py
tf.reset_default_graph()
dqn_runner = run_experiment.create_runner(DQN_PATH, schedule='continuous_train')
dqn_runner.run_experiment() 
```

ç„¶åæˆ‘ä»¬å¯ä»¥è¿è¡Œä¸€äº›ä¸ä¹‹å‰ç±»ä¼¼çš„ä»£ç æ¥ç”Ÿæˆä¸€ä¸ªä¸é”™çš„è§†é¢‘:

```py
rec = VideoRecorder(dqn_runner._environment.environment)
action = dqn_runner._initialize_episode()
rec.capture_frame()
while True:
    observation, reward, is_terminal = dqn_runner._run_one_step(action)
    rec.capture_frame()
    if is_terminal:
      break                                         # If the pole has fallen, quit.
    else:
      action = dqn_runner._agent.step(reward, observation)
dqn_runner._end_episode(reward)
rec.close() 
```

[https://winder.ai/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/dopamine.mp4](https://winder.ai/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/dopamine.mp4)

### [RLLib](https://ray.readthedocs.io/en/latest/rllib.html) é€šè¿‡[å°„çº¿æŠ•å°„](https://github.com/ray-project/ray)

![](img/17cdd05c98ff21df0db3b7283bb9218d.png)![](img/f2fe4f67d95ab34c1d289fbe4525c820.png)![](img/32dca77486c7b7b1b00a45bb80bc4202.png)![](img/05aafe063d49413de74dfc7d678b41bd.png)

Ray [å¼€å§‹æ—¶ï¼Œlife](https://github.com/ray-project/ray/tree/ray-0.3.0) æ˜¯ä¸€ä¸ªæ—¨åœ¨å¸®åŠ© Python ç”¨æˆ·æ„å»ºå¯æ‰©å±•è½¯ä»¶çš„é¡¹ç›®ï¼Œä¸»è¦ç”¨äº ML ç›®çš„ã€‚ä»é‚£æ—¶èµ·ï¼Œå®ƒæ·»åŠ äº†å‡ ä¸ªæ¨¡å—ï¼Œä¸“é—¨ç”¨äºç‰¹å®šçš„ ML ç”¨ä¾‹ã€‚ä¸€ä¸ªæ˜¯[åˆ†å¸ƒå¼è¶…å‚æ•°è°ƒè°](https://ray.readthedocs.io/en/latest/tune.html)ï¼Œå¦ä¸€ä¸ªæ˜¯[åˆ†å¸ƒå¼ RL](https://ray.readthedocs.io/en/latest/rllib.html) ã€‚

è¿™ç§ä¸€èˆ¬åŒ–çš„ç»“æœæ˜¯ï¼Œæµè¡Œæ•°å­—å¯èƒ½æ›´å¤šåœ°æ˜¯ç”±äºè¶…å‚æ•°å’Œé€šç”¨å¯ä¼¸ç¼©æ€§ç”¨ä¾‹ï¼Œè€Œä¸æ˜¯ RLã€‚æ­¤å¤–ï¼Œåº“çš„åˆ†å¸ƒå¼ç„¦ç‚¹æ„å‘³ç€ä»£ç†å®ç°å¾€å¾€æ˜¯å›ºæœ‰åˆ†å¸ƒå¼çš„(ä¾‹å¦‚ A3C ),æˆ–è€…è¯•å›¾è§£å†³å¦‚æ­¤å¤æ‚çš„é—®é¢˜ï¼Œä»¥è‡³äºå®ƒä»¬éœ€è¦åˆ†å¸ƒï¼Œä»è€Œä¸éœ€è¦å‡ å¹´å°±å¯ä»¥æ”¶æ•›(ä¾‹å¦‚ Rainbow)ã€‚

å°½ç®¡å¦‚æ­¤ï¼Œå¦‚æœæ‚¨æ­£åœ¨å¯»æ±‚ RL çš„ç”Ÿäº§ï¼Œæˆ–è€…å¦‚æœæ‚¨ä¸ºäº†è¶…å‚æ•°è°ƒæ•´æˆ–ç¯å¢ƒæ”¹å–„è€Œå¤šæ¬¡é‡å¤åŸ¹è®­ï¼Œé‚£ä¹ˆä½¿ç”¨ ray æ¥æ‰©å¤§è§„æ¨¡å¹¶å‡å°‘åé¦ˆæ—¶é—´å¯èƒ½æ˜¯æœ‰æ„ä¹‰çš„ã€‚äº‹å®ä¸Šï¼Œè®¸å¤šå…¶ä»–æ¡†æ¶(å…·ä½“æ¥è¯´: [SLM-Lab](#slm-lab-https-github-com-kengz-slm-lab) å’Œ [RLgraph](#rlgraph-https-github-com-rlgraph-rlgraph) )å®é™…ä¸Šéƒ½åœ¨å¹•åä½¿ç”¨äº† rayã€‚

æˆ‘ç›¸ä¿¡è¿™é‡Œå¯¹ RL æœ‰å¾ˆå¼ºçš„é€‚ç”¨æ€§ã€‚å¯¹åˆ†å¸ƒå¼è®¡ç®—çš„æ˜ç¡®å…³æ³¨æ˜¯å¥½çš„ã€‚æäº¤å’Œè´¡çŒ®è€…çš„ç»å¯¹æ•°é‡ä¹Ÿä»¤äººæ”¾å¿ƒã€‚ä½†æ˜¯ C++ä¸­æœ‰å¾ˆå¤šåº•å±‚ä»£ç ã€‚æœ‰äº›ç”šè‡³æ˜¯ Java è¯­è¨€ã€‚åªæœ‰ 60%æ˜¯ pythonã€‚

å°½ç®¡å¦‚æ­¤ï¼Œ[`Policy`ã€sã€‘](https://github.com/ray-project/ray/blob/master/python/ray/rllib/policy/policy.py)è¿˜æ˜¯æœ‰ä¸€ä¸ªéå¸¸æ¸…æ™°çš„æŠ½è±¡ï¼Œä¸€ä¸ªæ¼‚äº®çš„ï¼Œå‡ ä¹æ˜¯åŠŸèƒ½æ€§çš„ä»£ç†æ¥å£ï¼Œå«åš [`Trainer` s](https://github.com/ray-project/ray/blob/master/python/ray/rllib/agents/trainer.py) (å‚è§ [DQN å®ç°](https://github.com/ray-project/ray/blob/master/python/ray/rllib/agents/dqn/dqn.py)çš„ç”¨æ³•ç¤ºä¾‹)ï¼Œä¸€ä¸ª [`Model`](https://github.com/ray-project/ray/blob/master/python/ray/rllib/models/model.py) æŠ½è±¡ï¼Œå…è®¸ä½¿ç”¨ PyTorch æˆ– Tensorflow(è€¶ï¼)ä»¥åŠæ›´å¤šç”¨äºè¯„ä¼°å’Œæ”¿ç­–ä¼˜åŒ–çš„å·¥å…·ã€‚

æ€»çš„æ¥è¯´ï¼Œæ–‡æ¡£éå¸¸å‡ºè‰²ï¼Œå¹¶ä¸”å±•ç¤ºäº†æ¸…æ™°çš„å»ºç­‘å›¾çº¸(ä¾‹å¦‚ï¼Œå‚è§[æœ¬ä¾‹](https://ray.readthedocs.io/en/latest/rllib-models.html))ã€‚å®ƒæ˜¯æ¨¡å—åŒ–çš„ï¼Œå¯ä¼¸ç¼©æ€§å¥½ï¼Œå¾—åˆ°äº†ç¤¾åŒºçš„å¾ˆå¥½æ”¯æŒå’Œæ¥å—ã€‚å”¯ä¸€çš„ç¼ºç‚¹æ˜¯å®ƒçš„å¤æ‚æ€§ã€‚è¿™æ˜¯æ‚¨ä¸ºæ‰€æœ‰è¿™äº›åŠŸèƒ½ä»˜å‡ºçš„ä»£ä»·ã€‚

#### å…¥é—¨æŒ‡å—

Google colab é¢„è£…çš„ pyarrow ç‰ˆæœ¬å­˜åœ¨ä¸€ä¸ªä¸ ray ä¸å…¼å®¹çš„é—®é¢˜ã€‚ä½ å¿…é¡»å¸è½½é¢„è£…ç‰ˆæœ¬ï¼Œå¹¶é‡æ–°å¯åŠ¨è¿è¡Œæ—¶ï¼Œç„¶åå®ƒçš„å·¥ä½œã€‚

æˆ‘ä¹Ÿä¸èƒ½è®©è§†é¢‘æ¸²æŸ“åƒæˆ‘ä»¬ä¹‹å‰çš„ä¾‹å­ä¸€æ ·å·¥ä½œã€‚æˆ‘çš„å‡è®¾æ˜¯ï¼Œå› ä¸ºå®ƒä»¬è¿è¡Œåœ¨ä¸åŒçš„è¿›ç¨‹ä¸­ï¼Œæ‰€ä»¥å®ƒä»¬æ— æ³•è®¿é—®å‡çš„`pyvirtualdisplay`è®¾å¤‡ã€‚

å°½ç®¡å¦‚æ­¤ï¼Œè®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªä¾‹å­:

```py
!pip uninstall -y pyarrow
!pip install tensorflow ray[rllib] > /dev/null 2>&1 
```

åˆ é™¤ pyarrow å¹¶å®‰è£… rllib åï¼Œå¿…é¡»é‡å¯ç¬”è®°æœ¬å†…æ ¸ã€‚æ¥ä¸‹æ¥ï¼Œå¯¼å…¥å…‰çº¿:

```py
import ray
from ray import tune

ray.init() 
```

å¹¶ä½¿ç”¨ DQN ä¸º Cartpole ç¯å¢ƒè¿è¡Œè¶…å‚æ•°è°ƒæ•´ä½œä¸š:

```py
tune.run(
    "DQN",
    stop={"episode_reward_mean": 100},
    config={
        "env": "CartPole-v0",
        "num_gpus": 0,
        "num_workers": 1,
        "lr": tune.grid_search([0.01, 0.001, 0.0001]),
        "monitor": False,
    },
) 
```

è¿™é‡Œæœ‰å¾ˆå¤šè¯­æ³•ä¸Šçš„ç”œè¨€èœœè¯­ï¼Œä½†æ˜¯å®šåˆ¶åŸ¹è®­åŠŸèƒ½([æ–‡æ¡£](https://ray.readthedocs.io/en/latest/rllib-training.html#custom-training-workflows))çœ‹èµ·æ¥[ç›¸å½“ç®€å•](https://github.com/ray-project/ray/blob/master/python/ray/rllib/examples/custom_train_fn.py)ã€‚

### [Keras-RL](https://github.com/keras-rl/keras-rl)

![](img/d046d625387aa70cf7f6334b103e265d.png)![](img/21e9cd3f53e28efb70cb7a7e93d1feac.png)![](img/9f86cff664888a424d8d9c2baff69075.png)![](img/31ea3db8deeb7064d137cb7448ac8486.png)

æˆ‘çˆ± Kerasã€‚æˆ‘å–œæ¬¢æŠ½è±¡ï¼Œç®€å•ï¼Œåé”å®šã€‚å½“ä½ çœ‹ä¸‹é¢çš„ä»£ç æ—¶ï¼Œä½ å¯ä»¥çœ‹åˆ° Keras çš„é­”åŠ›ã€‚æ‰€ä»¥ä½ ä¼šè®¤ä¸º keras-rl æ˜¯ä¸€ä¸ªå®Œç¾çš„é€‰æ‹©ã€‚ç„¶è€Œï¼Œå®ƒä¼¼ä¹æ²¡æœ‰è·å¾—åƒå…¶ä»–æ¡†æ¶é‚£æ ·å¤šçš„å…³æ³¨ã€‚å¦‚æœä½ [çœ‹æ–‡æ¡£](https://keras-rl.readthedocs.io/en/latest/)ï¼Œå®ƒæ˜¯ç©ºçš„ã€‚å½“ä½ çœ‹åˆ°è¿™äº›æ‰¿è¯ºæ—¶ï¼Œåªæœ‰å°‘æ•°å‹‡æ•¢çš„äººå®Œæˆäº†å¤§éƒ¨åˆ†å·¥ä½œã€‚ä¸[ä¸»è¦çš„ Keras é¡¹ç›®](https://github.com/keras-team/keras)ç›¸æ¯”ã€‚

æˆ‘æƒ³æˆ‘å¯èƒ½çŸ¥é“åŸå› ã€‚Keras æ˜¯ä»å¤´å¼€å§‹æ„å»ºçš„ï¼Œå…è®¸ç”¨æˆ·å¿«é€ŸåŸå‹åŒ–ä¸åŒçš„ DL ç»“æ„ã€‚è¿™ä¾èµ–äºç¥ç»ç½‘ç»œåŸè¯­å¯ä»¥è¢«æŠ½è±¡å’Œæ¨¡å—åŒ–çš„äº‹å®ã€‚ä½†æ˜¯å½“ä½ çœ‹ keras-rl çš„ä»£ç æ—¶ï¼Œå®ƒçš„å®ç°æ–¹å¼å°±åƒæ•™ç§‘ä¹¦ä¸­ä¸€æ ·ã€‚ä¾‹å¦‚ï¼Œå°½ç®¡ SARSA å’Œ DQN ä¹‹é—´æœ‰ç›¸ä¼¼ä¹‹å¤„ï¼Œä½†æ¯ä¸ªä»£ç†éƒ½æœ‰è‡ªå·±çš„å®ç°ã€‚æƒ³æƒ³æ‰€æœ‰å¯ä»¥æ¨¡å—åŒ–çš„â€œæŠ€å·§â€ï¼Œå°±åƒå½©è™¹ä¸­ä½¿ç”¨çš„é‚£äº›æŠ€å·§ï¼Œå®ƒå¯ä»¥è®©äººä»¬åœ¨å…¶ä»–ä»£ç†ä¸­è¯•éªŒä½¿ç”¨è¿™äº›æŠ€å·§ã€‚æœ‰ä¸€å®šç¨‹åº¦çš„æ¨¡å—åŒ–ï¼Œä½†æˆ‘è®¤ä¸ºè¿™æ˜¯åœ¨ä¸€ä¸ªå¤ªé«˜çš„æ°´å¹³ã€‚

ä½†ä¹Ÿè®¸è¿˜ä¸ç®—å¤ªæ™šï¼Œå› ä¸ºè¿™é‡Œæœ‰å¤ªå¤šçš„å¸Œæœ›ã€‚å¦‚æœæœ‰è¶³å¤Ÿå¤šçš„äººæ„Ÿå…´è¶£ï¼Œæˆ–è€…å¦‚æœæœ‰æ›´å¤šæ¥è‡ªæ ¸å¿ƒ Keras é¡¹ç›®çš„æ”¯æŒï¼Œé‚£ä¹ˆè¿™å¯èƒ½æ˜¯æœªæ¥çš„é¦–é€‰ RL æ¡†æ¶ã€‚ä½†ç›®å‰æ¥çœ‹ï¼Œæˆ‘è§‰å¾—ä¸æ˜¯ã€‚é€šè¿‡ä½¿ç”¨æˆ‘ä»¬å·²ç»è®¨è®ºè¿‡çš„å…¶ä»–æ¡†æ¶ï¼Œå‡ ä¹åŒæ ·å®¹æ˜“è·å¾— Keras çš„å¥½å¤„ã€‚

#### å…¥é—¨æŒ‡å—

è¿™é‡Œçš„ä¾‹å­æ˜¯å¼€ç®±å³ç”¨çš„ï¼Œæˆ‘æ‰€åšçš„å”¯ä¸€ä¿®æ”¹æ˜¯ä½¿ç”¨æ¨¡æ‹Ÿæ˜¾ç¤ºå’Œæ·»åŠ ä¸€äº›æµ‹è¯•è§†é¢‘è®°å½•ã€‚å¯ä»¥çœ‹åˆ°è¿™é‡Œçš„å¤§éƒ¨åˆ†ä»£ç éƒ½æ˜¯æ ‡å‡†çš„ Keras ä»£ç ã€‚Keras-RL æ·»åŠ çš„å†…å®¹ä¸ Keras å®Œå…¨æ²¡æœ‰å…³ç³»ã€‚

```py
import numpy as np
import gym

from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten
from keras.optimizers import Adam

from rl.agents.dqn import DQNAgent
from rl.policy import BoltzmannQPolicy
from rl.memory import SequentialMemory

ENV_NAME = 'CartPole-v0'

# Get the environment and extract the number of actions.
env = gym.make(ENV_NAME)
np.random.seed(123)
env.seed(123)
nb_actions = env.action_space.n

# Next, we build a very simple model.
model = Sequential()
model.add(Flatten(input_shape=(1,) + env.observation_space.shape))
model.add(Dense(16))
model.add(Activation('relu'))
model.add(Dense(16))
model.add(Activation('relu'))
model.add(Dense(16))
model.add(Activation('relu'))
model.add(Dense(nb_actions))
model.add(Activation('linear'))
print(model.summary())

# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and
# even the metrics!
memory = SequentialMemory(limit=5000, window_length=1)
policy = BoltzmannQPolicy()
dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,
               target_model_update=1e-2, policy=policy)
dqn.compile(Adam(lr=1e-3), metrics=['mae'])

# Okay, now it's time to learn something! We visualize the training here for show, but this
# slows down training quite a lot. You can always safely abort the training prematurely using
# Ctrl + C.
dqn.fit(env, nb_steps=2500, visualize=True, verbose=2)

# After training is done, we save the final weights.
dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)

# Finally, evaluate our algorithm for 5 episodes.
dqn.test(Monitor(env, '.'), nb_episodes=5, visualize=True) 
```

[https://winder.ai/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/keras-rl.mp4](https://winder.ai/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/keras-rl.mp4)

### [TRFL](https://github.com/deepmind/trfl)

![](img/cec6aa55decccaac2bf838d6ff1a8278.png)![](img/75964f717174f6695c105f1f3301cb1c.png)![](img/8d300d5a27731abb9a5b641901404c4d.png)![](img/76a8baf65b4fe3467648ffc51fd195be.png)

TRFL æ˜¯ Deepmind å¯¹ Tensorflow çš„è‡ªä»¥ä¸ºæ˜¯çš„æ‰©å±•(é‚£ä¹ˆ NOGPï¼›-) ).é‰´äºè¿™äº›è¯æ˜ï¼Œæ‚¨å¯èƒ½ä¼šè®¤ä¸ºå®ƒä¼šå¾ˆå—æ¬¢è¿ï¼Œä½†æ˜¯æ‚¨æ³¨æ„åˆ°çš„ç¬¬ä¸€ä»¶äº‹æ˜¯æ˜æ˜¾ç¼ºä¹æäº¤ã€‚ç„¶åé²œæ˜çš„[ç¼ºä¹å®ä¾‹å’Œ Tensorflow 2.0 æ”¯æŒ](https://github.com/deepmind/trfl/issues/17)ã€‚

ä¸»è¦é—®é¢˜æ˜¯å¤ªä½çº§äº†ã€‚å’Œ Keras-RL å®Œå…¨ç›¸åã€‚TRFL æä¾›çš„åŠŸèƒ½æ˜¯ä¸€äº›è¾…åŠ©å‡½æ•°ï¼Œä¾‹å¦‚ä¸€ä¸ª [q-learning value å‡½æ•°](https://github.com/deepmind/trfl/blob/master/trfl/action_value_ops.py#L40)ï¼Œå®ƒæ¥å—ä¸€ä¸ªå¸¦æœ‰æŠ½è±¡åç§°çš„ Tensorflow å¼ é‡çš„è´Ÿè½½ã€‚

#### å…¥é—¨æŒ‡å—

æ¨èå¿«é€Ÿçœ‹ä¸€ä¸‹[è¿™ä¸ªç¬”è®°æœ¬](https://colab.research.google.com/drive/1r_SGbDBzEaKeijJFExgPTOcaglZcD0-S#scrollTo=627LbtjyZmYX)ä¸ºä¾‹ã€‚ä½†æ˜¯æ³¨æ„ä»£ç å¾ˆä½çº§ã€‚

### [å¼ é‡åŠ›](https://github.com/tensorforce/tensorforce)

![](img/743b21945fe0578f5b498f5cc1a4e9e4.png)![](img/efc1bb23d816b24927c54bad51be06f4.png)![](img/01a3f092be35276ac28df8dcbedb6e06.png)![](img/8cf18a5d76f29086f571ee4b55f71f9c.png)

Tensorforce ä¸ [TRFL](#TRFL) æœ‰ç€ç›¸ä¼¼çš„ç›®æ ‡ã€‚å®ƒè¯•å›¾æŠ½è±¡ RL åŸè¯­ï¼ŒåŒæ—¶ä»¥ Tensorflow ä¸ºç›®æ ‡ã€‚é€šè¿‡ä½¿ç”¨ Tensorflowï¼Œæ‚¨å¯ä»¥è·å¾—ä½¿ç”¨ Tensorflow çš„æ‰€æœ‰å¥½å¤„ï¼Œå³å›¾å½¢æ¨¡å‹ã€æ›´ç®€å•çš„è·¨å¹³å°éƒ¨ç½²ã€‚

ä¸€ä¸ª [`Environment`](https://github.com/tensorforce/tensorforce/blob/master/tensorforce/environments/environment.py) ã€ [`Runner`](https://github.com/tensorforce/tensorforce/blob/master/tensorforce/execution/runner.py) ã€ [`Agent`](https://github.com/tensorforce/tensorforce/blob/master/tensorforce/agents/agent.py) ã€ [`Model`](https://github.com/tensorforce/tensorforce/blob/master/tensorforce/models/model.py) å››ä¸ªé«˜çº§æŠ½è±¡ã€‚è¿™äº›åŸºæœ¬ä¸Šå®Œæˆäº†æ‚¨æ‰€æœŸæœ›çš„ï¼Œä½†æ˜¯â€œæ¨¡å‹â€æŠ½è±¡ä¸æ˜¯æ‚¨é€šå¸¸ä¼šçœ‹åˆ°çš„ã€‚ä¸€ä¸ª`Model`ä½äºä¸€ä¸ª`Agent`ä¸­ï¼Œå®šä¹‰ä»£ç†çš„ç­–ç•¥ã€‚è¿™å¾ˆå¥½ï¼Œå› ä¸ºï¼Œä¾‹å¦‚ï¼Œæ ‡å‡†çš„[Q-å­¦ä¹ æ¨¡å‹](https://github.com/tensorforce/tensorforce/blob/master/tensorforce/models/q_model.py)å¯ä»¥è¢«[Q-å­¦ä¹  n æ­¥æ¨¡å‹](https://github.com/tensorforce/tensorforce/blob/master/tensorforce/models/q_nstep_model.py)è¦†ç›–ï¼Œåªæ”¹å˜ä¸€ä¸ªå°å‡½æ•°ã€‚è¿™æ­£æ˜¯æˆ‘åœ¨å¯»æ‰¾çš„ TRFL å’Œ Keras ä¹‹é—´çš„ä¸­é—´åœ°å¸¦ã€‚å®ƒæ˜¯ä»¥é¢å‘å¯¹è±¡çš„æ–¹å¼å®ç°çš„ï¼Œæœ‰äº›äººä¼šå–œæ¬¢ï¼Œæœ‰äº›äººä¸ä¼šã€‚ä½†è‡³å°‘æŠ½è±¡æ˜¯å­˜åœ¨çš„ã€‚

åƒè¿™æ ·çš„åº“ï¼Œæˆ–è€…ä»»ä½•ä»¥ DL ä¸ºä¸­å¿ƒçš„ RL åº“çš„ç¼ºç‚¹æ˜¯ï¼Œåº•å±‚çš„ DL æ¡†æ¶ä½¿å¾—å¾ˆå¤šä»£ç å˜å¾—å¤æ‚ã€‚è¿™é‡Œä¹Ÿæ˜¯ä¸€æ ·ã€‚ä¾‹å¦‚ï¼Œ[éšæœºæ¨¡å‹](https://github.com/tensorforce/tensorforce/blob/major-revision/tensorforce/core/models/random_model.py)ï¼Œä¹Ÿå°±æ˜¯é€‰æ‹©ä¸€ä¸ªéšæœºåŠ¨ä½œçš„æ¨¡å‹ï¼Œå®ƒéœ€è¦ä¸€è¡Œä»£ç ï¼Œæœ‰ 79 è¡Œé•¿ã€‚æˆ‘åœ¨è¿™é‡Œå¼€äº†ä¸€ç‚¹ç©ç¬‘(è®¸å¯è¯ï¼Œç±»æ ·æ¿ï¼Œæ¢è¡Œç¬¦ï¼Œç­‰ç­‰ã€‚)ä½†å¸Œæœ›ä½ èƒ½ç†è§£æˆ‘çš„è§‚ç‚¹ã€‚

è¿™ä¹Ÿæ„å‘³ç€æ²¡æœ‰â€œç®€å•â€RL ç®—æ³•çš„å®ç°ï¼Œå³é‚£äº›ä¸ä½¿ç”¨æ¨¡å‹çš„ç®—æ³•ã€‚ä¾‹å¦‚ç†µã€åœŸåŒªã€ç®€å• MDPsã€SARSAã€ä¸€äº›è¡¨æ ¼æ–¹æ³•ç­‰ã€‚åŸå› æ˜¯è¿™äº›æ¨¡å‹ä¸éœ€è¦ DL æ¡†æ¶ã€‚

æ€»ä¹‹ï¼Œæˆ‘è®¤ä¸ºæŠ½è±¡çš„å±‚æ¬¡æ˜¯æ­£ç¡®çš„ã€‚ä½†æ˜¯å°†è‡ªå·±å±€é™äº DL æ¡†æ¶çš„å¥½å¤„/é—®é¢˜ä¾ç„¶å­˜åœ¨ã€‚

è¯·æ³¨æ„ï¼Œè¿™æ˜¯åŸºäºç‰ˆæœ¬`0.4.3`çš„ï¼Œä¸€ä¸ªä¸»è¦çš„é‡å†™æ­£åœ¨è¿›è¡Œä¸­ã€‚

#### å…¥é—¨æŒ‡å—

å…¥é—¨çš„ä¾‹å­æ˜¯æ˜æ™ºçš„ã€‚æˆ‘ä»¬æ­£åœ¨åˆ›å»ºä¸€ä¸ªç¯å¢ƒã€ä¸€ä¸ªä»£ç†å’Œä¸€ä¸ªè·‘æ­¥è€…(å®é™…è¿›è¡Œè®­ç»ƒçš„ä¸œè¥¿)ã€‚ä»£ç†çš„è§„æ ¼æœ‰ç‚¹ä¸åŒã€‚å®ƒè®©æˆ‘æƒ³èµ·äº† [Dopamine](#google-dopamine-https-github-com-google-dopamine) Gin configï¼Œåªä¸è¿‡å®ƒä½¿ç”¨çš„æ˜¯æ ‡å‡† jsonã€‚åœ¨ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»ç¤ºä¾‹ç›®å½•ä¸­è·å–è¿™äº›è§„èŒƒï¼Œä½†æ˜¯æ‚¨å¯ä»¥æƒ³è±¡ä½¿ç”¨å®ƒä»¬è¿è¡Œè¶…å‚æ•°æœç´¢æ˜¯å¤šä¹ˆå®¹æ˜“ã€‚

```py
environment = OpenAIGym(
    gym_id="CartPole-v0",
    monitor=".",
    monitor_safe=False,
    monitor_video=10,
    visualize=True
)

with urllib.request.urlopen("https://raw.githubusercontent.com/tensorforce/tensorforce/master/examples/configs/dqn.json") as url:
  agent = json.loads(url.read().decode())
  print(agent)
with urllib.request.urlopen("https://raw.githubusercontent.com/tensorforce/tensorforce/master/examples/configs/mlp2_network.json") as url:
  network = json.loads(url.read().decode())
  print(network)

agent = Agent.from_spec(
  spec=agent,
  kwargs=dict(
    states=environment.states,
    actions=environment.actions,
    network=network
  )
)

runner = Runner(
    agent=agent,
    environment=environment,
    repeat_actions=1
)

runner.run(
    num_timesteps=200,
    num_episodes=200,
    max_episode_timesteps=200,
    deterministic=True,
    testing=False,
    sleep=None
)
runner.close() 
```

[https://winder.ai/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/tensorforce.mp4](https://winder.ai/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/tensorforce.mp4)

### [åœ°å¹³çº¿](https://github.com/facebookresearch/Horizon)

![](img/36b9071e232a951b6f729926b14c7c95.png)![](img/1d74510a1b0f7081ea50bc6c92a08fa2.png)![](img/6b33dd5d1f9084646e102bbd5c809c73.png)![](img/34db8017f77531c2345ecedbf10ddac2.png)

Horizon æ˜¯ä¸€ä¸ªæ¥è‡ªè„¸ä¹¦çš„æ¡†æ¶ï¼Œç”± PyTorch ä¸»å¯¼ã€‚å¦ä¸€ä¸ªä»¥æ•°å­—å›¾ä¹¦é¦†ä¸ºä¸­å¿ƒçš„å›¾ä¹¦é¦†ã€‚å¦å¤–:

> Horizon çš„ä¸»è¦ç”¨ä¾‹æ˜¯åœ¨æ‰¹å¤„ç†è®¾ç½®ä¸­è®­ç»ƒ RL æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯•å›¾åœ¨ç»™å®šè¾“å…¥æ•°æ®çš„æƒ…å†µä¸‹å­¦ä¹ æœ€ä½³ç­–ç•¥ã€‚

å› æ­¤ï¼Œåƒå…¶ä»–æ¡†æ¶ä¸€æ ·ï¼Œç„¦ç‚¹æ˜¯ä¸ç­–ç•¥æ— å…³çš„ã€æ¨¡å‹é©±åŠ¨çš„ RL å’Œæ¨¡å‹ä¸­çš„ DLã€‚ä½†æ˜¯ç”±äº PyTorch çš„ä½¿ç”¨ï¼Œè¿™æ˜¯æœ‰åŒºåˆ«çš„ã€‚æ‚¨ä¹Ÿå¯ä»¥å°†å®ƒä¸ä½¿ç”¨ PyTorch ä½œä¸º Keras åç«¯çš„ Keras-RL è¿›è¡Œæ¯”è¾ƒã€‚

æˆ‘å·²ç»åœ¨ [Tensorforce](#tensorforce-https-github-com-tensorforce-tensorforce) ä¸€èŠ‚ä¸­è®¨è®ºäº†è¿™ç§èšç„¦æ¡†æ¶çš„ç¼ºç‚¹ï¼Œæ‰€ä»¥æˆ‘ä¸å†èµ˜è¿°ã€‚

å°½ç®¡å¦‚æ­¤ï¼Œè¿˜æ˜¯æœ‰ä¸€äº›æœ‰è¶£çš„åŒºåˆ«ã€‚æ²¡æœ‰ç´§å¯†çš„å¥èº«æˆ¿æ•´åˆã€‚ç›¸åï¼Œä»–ä»¬é€šè¿‡å°†å¥èº«æˆ¿æ•°æ®è½¬å‚¨åˆ° JSON ä¸­ï¼Œç„¶åå°† JSON è¯»å›åˆ°ä»£ç†ä¸­ï¼Œæœ‰æ„åœ°å°†ä¸¤è€…åˆ†ç¦»ã€‚è¿™å¬èµ·æ¥å¯èƒ½æœ‰ç‚¹ç½—å—¦ï¼Œä½†å®é™…ä¸Šå¯¹[è§£è€¦](https://en.wikipedia.org/wiki/Loose_coupling)å¾ˆæœ‰å¥½å¤„ï¼Œå› æ­¤æ›´å…·å¯ä¼¸ç¼©æ€§ï¼Œä¸é‚£ä¹ˆè„†å¼±ï¼Œä¹Ÿæ›´çµæ´»ã€‚ä¸åˆ©çš„ä¸€é¢æ˜¯ï¼Œç”±äºå¤æ‚æ€§çš„å¢åŠ ï¼Œéœ€è¦è·¨è¶Šæ›´å¤šçš„éšœç¢ã€‚

ç„¶è€Œï¼Œå¬èµ·æ¥ä¸å¯æ€è®®çš„æ˜¯ï¼ŒHorizon æ²¡æœ‰ pip å®‰è£…ç¨‹åºã€‚ä½ è¦ç”¨ condaï¼Œå®‰è£… onnxï¼Œå®‰è£… javaï¼Œè®¾ç½®`JAVA_HOME`æŒ‡å‘ condaï¼Œå®‰è£… Sparkï¼Œå®‰è£… Gym(å¤Ÿå…¬å¹³)ï¼Œå®‰è£… Apache thrift ç„¶åæ„å»º Horizonã€‚å“‡å“¦ã€‚(å¦‚æœä½ æ•°äº†æœ‰å¤šå°‘æ­¥ï¼Œä¼šåŠ åˆ†)ã€‚

å› æ­¤ï¼Œæˆ‘è®¤ä¸ºè¿™è¶³ä»¥è¯´æ˜æˆ‘ä¸æ‰“ç®—åœ¨æ¼”ç¤ºç¬”è®°æœ¬ä¸Šå®‰è£…å®ƒã€‚

#### å…¥é—¨æŒ‡å—

ä½ éœ€è¦å¤§é‡çš„æ—¶é—´å’Œè€å¿ƒã€‚éµå¾ª[å»ºé€ è¯´æ˜](https://horizonrl.com/installation.html)ï¼Œç„¶åéµå¾ª[è®­ç»ƒæŒ‡å—](https://horizonrl.com/usage.html#offline-rl-training-batch-rl)ã€‚æˆ‘ä¸èƒ½ä¿è¯ï¼Œå› ä¸ºæˆ‘è¿˜æœ‰è‡ªå·±çš„ç”Ÿæ´»ã€‚

### [è”»é©°](https://github.com/NervanaSystems/coach)

![](img/6894610c42823ad50272b027897d5dc8.png)![](img/c034bc89d893b9d9c4e5b48cdcbe7962.png)![](img/3549dcfa497bb8a905fc22079515465b.png)![](img/91100d57f0d592d8da0713619f63a755.png)

å½“ä½ æŸ¥çœ‹è¿™ä¸ªæ¡†æ¶æ—¶ï¼Œä½ ä¼šæ³¨æ„åˆ°çš„ç¬¬ä¸€ä»¶äº‹æ˜¯å®ç°ç®—æ³•çš„æ•°é‡ã€‚è¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„å·¥ç¨‹ï¼Œè‚¯å®šéœ€è¦å‡ ä¸ªäººèŠ±è´¹æ•°å‘¨çš„æ—¶é—´æ¥å®Œæˆã€‚ä½ ä¼šæ³¨æ„åˆ°çš„ç¬¬äºŒä»¶äº‹æ˜¯é›†æˆç¯å¢ƒçš„[æ•°é‡](https://github.com/NervanaSystems/coach#supported-environments)ã€‚è€ƒè™‘åˆ°è¿™éœ€è¦å¤šå°‘æ—¶é—´ï¼Œå®ƒç»™äº†æ¡†æ¶çš„å…¶ä½™éƒ¨åˆ†å¾ˆå¤šå¸Œæœ›ã€‚

å®ƒé…æœ‰ä¸€ä¸ªçœ‹èµ·æ¥éå¸¸æ¼‚äº®çš„ä¸“ç”¨ä»ªè¡¨ç›˜ã€‚å…¶ä»–å¤§å¤šæ•°æ¡†æ¶éƒ½ä¾èµ– Tensorboard é¡¹ç›®ã€‚

ä¸€ä¸ªæˆ‘ä»¥å‰æ²¡æœ‰è§è¿‡çš„ wow ç‰¹æ€§æ˜¯ Kubernetes çš„å†…ç½®éƒ¨ç½²ã€‚æˆ‘è®¤ä¸ºè”»é©°å¯¹è”»é©°çš„ç¼–æ’èµ°å¾—å¤ªè¿œäº†ï¼Œä½†äº‹å®ä¸Šä»–ä»¬ç”šè‡³è€ƒè™‘è¿‡å®ƒï¼Œè¿™æ„å‘³ç€å®ƒå¯èƒ½è¶³å¤Ÿå¯ä¼¸ç¼©ï¼Œå¯ä»¥ç”¨æ ‡å‡†å·¥å…·éƒ¨ç½²åˆ° Kuberentes ä¸Šã€‚

æ¨¡å—åŒ–çš„ç¨‹åº¦ä»¤äººéœ‡æƒŠã€‚ä¾‹å¦‚ï¼Œæœ‰äº›ç±»å®ç°äº†å„ç§å„æ ·çš„[æ¢ç´¢ç­–ç•¥](https://nervanasystems.github.io/coach/components/exploration_policies/index.html)ï¼Œå¹¶å…è®¸ä½ å¯¹[å„ç§æ¨¡å‹è®¾è®¡](https://nervanasystems.github.io/coach/design/network.html)åšå‡ºå„ç§å„æ ·çš„æ”¹å˜ã€‚

æˆ‘èƒ½æƒ³åˆ°çš„å”¯ä¸€æœ‰ç‚¹çƒ¦äººçš„æ˜¯å¼ºè¿«æˆ‘ä½¿ç”¨ DL ä½œä¸ºæ¨¡å‹çš„é™åˆ¶ã€‚æˆ‘ä»ç„¶ç›¸ä¿¡ï¼Œæ›´ç®€å•çš„åº”ç”¨ç¨‹åºå­é›†ä¸éœ€è¦åƒ DL é‚£æ ·å¤æ‚çš„ä¸œè¥¿ï¼Œå¹¶ä¸”å¯ä»¥ä»æ›´ä¼ ç»Ÿçš„å›å½’æ–¹æ³•ä¸­å—ç›Šã€‚ç„¶è€Œï¼Œæˆ‘ç¡®ä¿¡æ·»åŠ ä¸€ä¸ªç§»é™¤ DL å†…å®¹çš„å°å­˜æ ¹ç±»åº”è¯¥æ˜¯ç›¸å½“å®¹æ˜“çš„ã€‚

æœ‰è¶£çš„æ˜¯ï¼Œæ¡†æ¶[æ”¯æŒ Tensorflow å’Œ MXNet](https://nervanasystems.github.io/coach/usage.html#switching-between-deep-learning-frameworks) ,å› ä¸ºå®ƒä½¿ç”¨äº† Kerasã€‚è¿™æ„å‘³ç€ä¸æ”¯æŒ PyTorchï¼Œå› ä¸º Keras ä¸æ”¯æŒ PyTorchã€‚

å¦ç™½åœ°è¯´ï¼Œæˆ‘ä¸èƒ½ç†è§£ä¸ºä»€ä¹ˆè¿™ä¸ªæ¡†æ¶åœ¨ä»»ä½•ä¸€ç§è¡¡é‡æ–¹å¼ä¸‹éƒ½å¦‚æ­¤ä¸å—æ¬¢è¿ã€‚å°±æ˜Ÿæ˜Ÿè€Œè¨€ã€‚å°±è°·æ­Œé¡µé¢çš„æ•°é‡è€Œè¨€(å¦‚æœä½ æƒ³çŸ¥é“çš„è¯ï¼Œç­”æ¡ˆæ˜¯ 7)ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ[è°·æ­Œå¤šå·´èƒº](https://www.google.com/search?q=%22google+dopamine%22)æœ‰ 16500 é¡µã€‚

å®ƒå½“ç„¶æ˜¯æœ€å…¨é¢çš„æ¡†æ¶ï¼Œæœ‰æœ€å¥½çš„æ–‡æ¡£å’Œæå¥½çš„æ¨¡å—åŒ–æ°´å¹³ã€‚ä»–ä»¬ç”šè‡³æœ‰ä¸€æœ¬â¤ï¸ [å…¥é—¨ç¬”è®°æœ¬](https://github.com/NervanaSystems/coach/blob/v0.12.1/tutorials/0.%20Quick%20Start%20Guide.ipynb) â¤ï¸.

#### å…¥é—¨æŒ‡å—

æˆ‘æƒ³æŒ‡å‡ºä¸¤ä¸ªé‡è¦çš„æ³¨æ„äº‹é¡¹ã€‚é¦–å…ˆï¼Œç¡®ä¿æ‚¨çœ‹åˆ°çš„æ˜¯æ–‡æ¡£æˆ–æ¼”ç¤ºçš„æ ‡è®°ç‰ˆæœ¬ã€‚åœ¨ master åˆ†æ”¯ä¸­æœ‰ä¸€äº›æ–°åŠŸèƒ½ä¸èƒ½ä¸å®‰è£…äº† pip çš„ç‰ˆæœ¬ä¸€èµ·ä½¿ç”¨ã€‚ç¬¬äºŒï¼Œä¾èµ–äº OpenAI å¥èº«æˆ¿ç‰ˆæœ¬`0.12.5`ï¼Œcolab é‡Œæ²¡æœ‰å®‰è£…ã€‚æ‚¨éœ€è¦è¿è¡Œ`!pip install gym==0.12.5`å¹¶é‡å¯è¿è¡Œæ—¶ã€‚

```py
import tensorflow as tf
tf.reset_default_graph() # So that we don't get an error for TF when we re-run

from rl_coach.agents.clipped_ppo_agent import ClippedPPOAgentParameters
from rl_coach.environments.gym_environment import GymVectorEnvironment
from rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager
from rl_coach.graph_managers.graph_manager import ScheduleParameters
from rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps
from rl_coach.base_parameters import VisualizationParameters
global experiment_path; experiment_path = '.' # Because of some bizzare global in the mp4 dumping code

# Custom schedule to speed up training. We don't really care about the results.
schedule_params = ScheduleParameters()
schedule_params.improve_steps = TrainingSteps(200)
schedule_params.steps_between_evaluation_periods = EnvironmentSteps(200)
schedule_params.evaluation_steps = EnvironmentEpisodes(10)
schedule_params.heatup_steps = EnvironmentSteps(0)

graph_manager = BasicRLGraphManager(
    agent_params=ClippedPPOAgentParameters(),
    env_params=GymVectorEnvironment(level='CartPole-v0'),
    schedule_params=schedule_params,
    vis_params=VisualizationParameters(dump_mp4=True) # So we can dump the video
) 
```

[https://winder.ai/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/rl_coach.mp4](https://winder.ai/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/rl_coach.mp4)

### [MAgent](https://github.com/geek-ai/MAgent)

![](img/e3226e9b0ddb42baca503aa6215e4c95.png)![](img/ea9346f7fd0000d95740dd177e51fb6f.png)![](img/cd12e48cf5d2abae76f2053db58d9200.png)![](img/019dbebd29219abe1a9415132ae91bff.png)

MAgent æ˜¯ä¸€ä¸ªå…è®¸ä½ è§£å†³å¤šä»£ç† RL é—®é¢˜çš„æ¡†æ¶ã€‚ä¸æ‰€æœ‰å…¶ä»–ä»…ä½¿ç”¨å•ä¸ªæˆ–éå¸¸å°‘çš„ä»£ç†çš„â€œä¼ ç»Ÿâ€RL æ¡†æ¶ç›¸æ¯”ï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨ä¸åŒçš„ç›®æ ‡ã€‚ä»–ä»¬å£°ç§°å®ƒå¯ä»¥æ‰©å±•åˆ°æ•°ç™¾ä¸‡ä¸ªä»£ç†ã€‚

ä½†åŒæ ·ï¼Œæ²¡æœ‰ pip å®‰è£…ç¨‹åºã€‚è¯·å¤§å®¶ä¸ºè‡ªå·±çš„é¡¹ç›®åˆ›å»º pip å®‰è£…ç¨‹åºã€‚è¿™å¯¹äºæ˜“ç”¨æ€§å’Œé¡¹ç›®å¸å¼•åŠ›è‡³å…³é‡è¦ã€‚æˆ‘çŒœæ˜¯å› ä¸ºæ•´ä¸ªé¡¹ç›®éƒ½æ˜¯ç”¨ C å†™çš„ï¼Œå¤§æ¦‚æ˜¯æ€§èƒ½åŸå› å§ã€‚

å®ƒåœ¨å¼•æ“ç›–ä¸‹ä½¿ç”¨ Tensorflowï¼Œå¹¶æ„å»ºè‡ªå·±çš„ gridworld å¼ç¯å¢ƒã€‚ä»£ç†çš„è®¾è®¡è€ƒè™‘äº†â€œçœŸå®ç”Ÿæ´»â€çš„æ¨¡æ‹Ÿã€‚ä¾‹å¦‚ä½ å¯ä»¥æŒ‡å®šä»£ç†çš„å¤§å°ï¼Œå®ƒèƒ½çœ‹å¤šè¿œï¼›è¯¸å¦‚æ­¤ç±»çš„äº‹æƒ…ã€‚ä¼ é€’ç»™ä»£ç†çš„è§‚å¯Ÿç»“æœæ˜¯ç½‘æ ¼ã€‚ä»–ä»¬èƒ½é‡‡å–çš„è¡ŒåŠ¨ä»…é™äºç§»åŠ¨ã€æ”»å‡»å’Œè½¬èº«ã€‚ä»–ä»¬æ ¹æ®çµæ´»çš„è§„åˆ™å®šä¹‰è·å¾—å¥–åŠ±ã€‚

ç®€è€Œè¨€ä¹‹ï¼Œè¯¥æ¡†æ¶è¢«è®¾ç½®ä¸ºå¤„ç†å¼€ç®±å³ç”¨çš„ç”Ÿæ´»æ–¹å¼æ¸¸æˆï¼Œå¹¶åœ¨ä»£ç†å¦‚ä½•è¡Œä¸ºå’Œå¥–åŠ±æ–¹é¢å…·æœ‰ä¸€äº›é¢å¤–çš„æ¨¡å—åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€äº›æ›´é«˜çº§çš„ DL æ–¹æ³•æ¥è®­ç»ƒä»£ç†æ‰§è¡Œå¤æ‚ã€åè°ƒçš„ä»»åŠ¡ã€‚æ¯”å¦‚å›´ä½çŒç‰©è®©å®ƒä¸èƒ½åŠ¨ã€‚æ‚¨å¯ä»¥åœ¨[å…¥é—¨æŒ‡å—](https://github.com/geek-ai/MAgent/blob/master/doc/get_started.md)ä¸­äº†è§£æ›´å¤šä¿¡æ¯ã€‚

è¿™ä¸ªæƒ³æ³•ç»™æˆ‘ç•™ä¸‹äº†æ·±åˆ»çš„å°è±¡ã€‚ä½†æ˜¯ä½ å¯ä»¥ä»ä¸Šé¢çš„ Github ç»Ÿè®¡æ•°æ®ä¸­çœ‹åˆ°ï¼Œä¸€å¹´ 4 æ¬¡æäº¤åŸºæœ¬ä¸Šæ„å‘³ç€å®ƒå¾ˆå°‘è¢«ä½¿ç”¨ã€‚æœ€è¿‘ä¸€æ¬¡é‡å¤§æ›´æ–°æ˜¯åœ¨ 2017 å¹´ã€‚è¿™æ˜¯ä¸€ä¸ªé—æ†¾ï¼Œå› ä¸ºä¸å…¶ä»–æ¡†æ¶ç›¸æ¯”ï¼Œå®ƒä»£è¡¨äº†éå¸¸ä¸åŒçš„ä¸œè¥¿ã€‚å¦‚æœæœ‰äººèƒ½è®©å®ƒæ›´å®¹æ˜“ä½¿ç”¨ï¼Œæˆ–è€…ç”¨æƒ¯ç”¨çš„ Python å¤åˆ¶æ¡†æ¶ï¼Œè¿™æ ·å®ƒå°±å˜å¾—æ›´å®¹æ˜“ä½¿ç”¨ï¼Œé‚£å°±å¤ªå¥½äº†ã€‚

#### å…¥é—¨æŒ‡å—

æ‰€ä»¥æˆ‘å‡ ä¹å¯ä»¥åœ¨ç¬”è®°æœ¬ä¸Šå·¥ä½œäº†ã€‚æˆ‘è¯•äº†å‡ ä¸ªæ¥è‡ª[å…¥é—¨æŒ‡å—](https://github.com/geek-ai/MAgent/blob/master/doc/get_started.md)çš„ä¾‹å­ã€‚è®­ç»ƒç‰ˆéœ€è¦å‡ ä¸ªå°æ—¶ï¼Œæ‰€ä»¥æˆ‘å¾ˆå¿«å°±æ”¾å¼ƒäº†ã€‚ç„¶è€Œ,`examples/api_demo.py`åªæ˜¯åœ¨æµ‹è¯•å­¦ä¹ è¿‡çš„æ¨¡å‹ï¼Œæ‰€ä»¥é€Ÿåº¦éå¸¸å¿«ã€‚

ä½†æ˜¯ï¼Œå®ƒä»¥æŸç§ä¸“æœ‰çš„æ–‡æœ¬æ ¼å¼å‘ˆç°ç¯å¢ƒã€‚æ‚¨éœ€è¦è¿è¡Œä¸€ä¸ªéšæœºçš„ webserver äºŒè¿›åˆ¶æ–‡ä»¶æ¥è§£æå’Œæ‰˜ç®¡æµè§ˆå™¨ä¸­çš„æ¸²æŸ“ã€‚å› ä¸ºæˆ‘ä»¬åœ¨ colabï¼Œå®ƒä¸å…è®¸ä½ è¿è¡Œç½‘ç»œæœåŠ¡å™¨ã€‚æ‰€ä»¥æˆ‘è¯•ç€ä¸‹è½½æ–‡ä»¶ï¼Œä½†æˆ‘ä»¬åœ¨ colab ä¸Šæ„å»ºäº†äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œè€Œä¸æ˜¯åœ¨ mac ä¸Šï¼Œæ‰€ä»¥æˆ‘æ— æ³•è¿è¡ŒäºŒè¿›åˆ¶æ–‡ä»¶ã€‚

æ‰€ä»¥è¿™æœ‰ç‚¹ä»¤äººæ²®ä¸§ã€‚å¦‚æœå®ƒåªæ˜¯ä»¥æŸç§æ ‡å‡†æ ¼å¼(å¦‚ mp4 æ ¼å¼æˆ– gif æ ¼å¼)å‘ˆç°å®ƒï¼Œäº‹æƒ…ä¼šç®€å•å¾—å¤šã€‚ä¹Ÿä»¤äººå¤±æœ›ï¼Œå› ä¸ºæˆ‘æœŸå¾…ç”Ÿæˆä¸€äº›å¤æ‚çš„è¡Œä¸ºã€‚

ä½†æ˜¯ä¸ºäº†ä¸è®©ä½ å¤±æœ›ï¼Œè¿™é‡Œæœ‰ä¸€äº›ä½œè€…çš„èµå¿ƒæ‚¦ç›®çš„ä¸œè¥¿ã€‚è¯·åŸè°…éŸ³é¢‘ï¼

[https://www.youtube.com/embed/HCSm0kVolqI](https://www.youtube.com/embed/HCSm0kVolqI)

ä»¥ä¸‹æ˜¯ä»ç„¶æœ‰æ•ˆçš„ä»£ç :

```py
!git clone https://github.com/geek-ai/MAgent.git
!sudo apt-get install cmake libboost-system-dev libjsoncpp-dev libwebsocketpp-dev
%cd MAgent
!bash build.sh

!PYTHONPATH=$(pwd)/python:$PYTHONPATH python examples/api_demo.py 
```

æ‚¨å¯ä»¥å°†æœ€åä¸€æ¬¡è°ƒç”¨äº¤æ¢åˆ° examples æ–‡ä»¶å¤¹ä¸­çš„ä»»ä½• python æ–‡ä»¶ã€‚

### [TF-Agents](https://github.com/tensorflow/agents)

![](img/f409199a8d7ae8a20ac8b42f90c65ac2.png)![](img/8c9a85c2b8ca73323c91ee8ad5890842.png)![](img/95b8e09177beef49b60a44b4fdb22947.png)![](img/bc65284999b299940393d982b48a4798.png)

Tensorflow-Agents (TF-Agents)æ˜¯è°·æ­Œçš„å¦ä¸€ä¸ª [NOGP](#google-dopamine-https-github-com-google-dopamine) ï¼Œä¸“æ³¨äº Tensorflowã€‚æ‰€ä»¥æŠŠè¿™å½“æˆæ˜¯å¯¹ [TRFL](#trfl-https-github-com-deepmind-trfl) ã€ [Tensorforce](#tensorforce-https-github-com-tensorforce-tensorforce) å’Œ[å¤šå·´èƒº](#google-dopamine-https-github-com-google-dopamine)çš„ç›´æ¥ç«äº‰ã€‚

è¿™å°±å¼•å‡ºäº†ä¸€ä¸ªé—®é¢˜:å½“ TRFL å’Œå¤šå·´èƒºå·²ç»å­˜åœ¨çš„æ—¶å€™ï¼Œä¸ºä»€ä¹ˆæ›´å¤šçš„è°·æ­Œå‘˜å·¥åˆ›é€ äº†å¦ä¸€ä¸ª Tensorflow-abstraction-for-RLï¼Ÿåœ¨è®¨è®º TF-Agents å’Œå¤šå·´èƒºä¹‹é—´å…³ç³»çš„ä¸€æœŸ[ä¸­ï¼Œæ’°ç¨¿äººå»ºè®®:](https://github.com/tensorflow/agents/issues/15)

> ä¼¼ä¹å¤šå·´èƒºå’Œ TF-Agent å¼ºçƒˆé‡å ã€‚è™½ç„¶å¤šå·´èƒºæ—¨åœ¨ç”¨äºå¿«é€ŸåŸå‹å’ŒåŸºå‡†æµ‹è¯•ï¼Œå› ä¸ºå†ç°æ€§å·²è¢«ç½®äºé¡¹ç›®çš„æ ¸å¿ƒï¼Œè€Œ TF-Agent å°†æ›´å¤šåœ°ç”¨äºç”Ÿäº§çº§å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚

è¯´å®è¯ï¼Œæˆ‘ç°åœ¨è¿˜ä¸ç¡®å®šâ€œç”Ÿäº§çº§â€æ˜¯ä»€ä¹ˆæ„æ€ã€‚æœ‰ä¸€äº›[å¾ˆæ£’çš„ colab ä¾‹å­](https://github.com/tensorflow/agents/tree/master/tf_agents/colabs)ï¼Œä½†æ˜¯æ²¡æœ‰æ–‡æ¡£ã€‚ä½ å½“ç„¶ä¸åº”è¯¥åœ¨ç”Ÿäº§ä¸­ä½¿ç”¨ç¬”è®°æœ¬ç”µè„‘ã€‚

ä¸€æ—¦ä½ å¼€å§‹æ·±å…¥ç ”ç©¶è¿™äº›ä¾‹å­ï¼Œä½ å°±ä¼šå‘ç°è¿™äº›ä»£ç æ˜¯éå¸¸å¼ é‡æµçš„ã€‚ä¾‹å¦‚ï¼Œ[ç®€å•çš„ Cartpole ç¤ºä¾‹](https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb)æœ‰å¾ˆå¤šè¡Œä»£ç ã€‚ä¸»è¦æ˜¯å› ä¸ºé‡Œé¢æœ‰å¾ˆå¤šè§£é‡Šå’Œè°ƒè¯•ä»£ç ï¼Œä½†å®ƒçœ‹èµ·æ¥åƒæ˜¯å³å°†åˆ°æ¥çš„äº‹æƒ…çš„è¿¹è±¡ã€‚

ä¸è¿‡ï¼Œæˆ‘å¿…é¡»æ‰¿è®¤ï¼Œä»£ç çœ‹èµ·æ¥ç¡®å®éå¸¸å¥½ã€‚å®ƒè¢«å¾ˆå¥½åœ°åˆ†å¼€ï¼Œæ¨¡å—åŒ–çœ‹èµ·æ¥å¾ˆå¥½ã€‚ä½ æ‰€æœŸæœ›çš„æ‰€æœ‰æŠ½è±¡éƒ½åœ¨é‚£é‡Œã€‚æˆ‘å”¯ä¸€æƒ³æŒ‘é€‰çš„æ˜¯ [`Agent`æŠ½è±¡](https://github.com/tensorflow/agents/blob/master/tf_agents/agents/tf_agent.py)ã€‚è¿™æ˜¯åŸºç±»ï¼Œå®ƒç›´æ¥è€¦åˆåˆ° Tensorflowã€‚è¿™æ˜¯ä¸€ä¸ªå¼ é‡æµæ¨¡å—ã€‚è¿™å¢åŠ äº†å¤§é‡çš„å¤æ‚æ€§ï¼Œæˆ‘å¸Œæœ›å®ƒè¢«æŠ½è±¡æ‰ï¼Œè¿™æ ·æˆ‘å°±ä¸å¿…æ‹…å¿ƒå®ƒï¼Œç›´åˆ°æˆ‘éœ€è¦å®ƒçš„æ—¶å€™ã€‚å¯¹äºç»å¤§å¤šæ•°å…¶ä»–æŠ½è±¡æ¥è¯´ä¹Ÿæ˜¯å¦‚æ­¤ï¼›éƒ½æ˜¯ Tensorflow æ¨¡å—ã€‚

ä¹Ÿå°±æ˜¯è¯´ï¼Œå¾ˆæ˜æ˜¾è¿™æ˜¯ä¸€ä¸ªæ¯” TRFL æ›´åŠ ä¸¥è‚ƒå’Œå¼ºå¤§çš„åº“ã€‚

#### å…¥é—¨æŒ‡å—

ä»–ä»¬çš„èµ„æºåº“ä¸­å·²ç»æœ‰ä¸€å¥—[å¹¿æ³›çš„ç¬”è®°æœ¬å¯ç”¨ï¼Œæ‰€ä»¥æˆ‘ä¸ä¼šæµªè´¹æ—¶é—´åœ¨è¿™é‡Œå¤åˆ¶å’Œç²˜è´´ã€‚ä½ ä¹Ÿå¯ä»¥ç›´æ¥åœ¨ colab](https://github.com/tensorflow/agents/tree/master/tf_agents/colabs)ä¸­è¿è¡Œå®ƒä»¬[ã€‚](https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb)

ä¸‹é¢çš„è§†é¢‘æ˜¾ç¤ºäº†ä¸‰é›†çš„ç¿»ç­‹æ–—ã€‚å¯¹æˆ‘æ¥è¯´ï¼Œå®ƒçœ‹èµ·æ¥åƒæœ‰ä¸€ä¸ªåˆé€‚çš„ã€‚ä¸æ–­æ¨åŠ¨å†å¹³è¡¡ã€‚

[https://winder.ai/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/tf-agents.mp4](https://winder.ai/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/tf-agents.mp4)

### [SLM-Lab](https://github.com/kengz/SLM-Lab)

![](img/79b34f2161df9de2fe83c9bf3cbbc66c.png)![](img/90a39fe7b7127cd33cdbc6c0986f382b.png)![](img/b8ae0eb94eef63406f7bfe5006372a5f.png)![](img/c7afa0f49b1b5c77450247a5aa7d07d4.png)

SLM-Lab æ˜¯åŸºäº PyTorch çš„æ¨¡å—åŒ– RL æ¡†æ¶ã€‚å®ƒä¼¼ä¹æ›´é¢å‘ç ”ç©¶äººå‘˜ã€‚ä»–ä»¬å¼ºè°ƒæ¨¡å—åŒ–çš„é‡è¦æ€§ï¼Œä½†æ˜¯[æ­£ç¡®åœ°æŒ‡å‡º](https://github.com/kengz/SLM-Lab#simplicity)ç®€å•å’Œæ¨¡å—åŒ–å¯èƒ½æ˜¯ä¸å¯èƒ½çš„ï¼›è¿™æ˜¯ä¸¤è€…ä¹‹é—´çš„å¦¥åã€‚æœ‰è¶£çš„æ˜¯ï¼Œå®ƒè¿˜ä½¿ç”¨[çš„å°„çº¿é¡¹ç›®](#rllib-https-ray-readthedocs-io-en-latest-rllib-html-via-ray-project-https-github-com-ray-project-ray)æ¥ä½¿å…¶å¯æ‰©å±•ã€‚

å°½ç®¡ä» 2017 å¹´å¼€å§‹ï¼Œè´¡çŒ®è€…æ•°é‡å¾ˆå°‘ï¼Œgithub æ˜æ˜Ÿç›¸å¯¹å—æ¬¢è¿ï¼Œä½†æ´»åŠ¨å¾ˆå¤šã€‚ç»å¤§å¤šæ•°æäº¤éƒ½æ˜¯ä¸€è¡Œç¨‹åºï¼Œä½†æ˜¯ä½œè€…çš„æ‰¿è¯ºæ˜¯æƒŠäººçš„ã€‚

ä¸å¹¸çš„æ˜¯ï¼Œè¿™æ˜¯å¦ä¸€ä¸ªé pip å®‰è£…æ¡†æ¶ï¼Œå¹¶è¯•å›¾å®‰è£…ä¸ C åº“å’Œ miniconda æ„å»ºç›¸å…³çš„å…¨éƒ¨è´Ÿè½½ã€‚è¿™åœ¨ colab æ˜¯æœ‰é—®é¢˜çš„ã€‚ä½œä¸ºä¸€åä¼˜ç§€çš„å·¥ç¨‹å¸ˆï¼Œæˆ‘å¿½ç•¥äº†æ‰€æœ‰çš„æ–‡æ¡£ï¼Œå¹¶è¯•å›¾é€šè¿‡åå¤è¯•éªŒè®©å®ƒè‡ªå·±å·¥ä½œã€‚å‡ ä¹æˆåŠŸäº†ï¼Œä½†æ˜¯æˆ‘åœ¨åˆå§‹åŒ– pytorch æ—¶å¶ç„¶å‘ç°äº†ä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä¸çŸ¥é“å¦‚ä½•è§£å†³ã€‚

å› æ­¤ï¼Œä¸å¹¸çš„æ˜¯ï¼Œä½ å°†ä¸å¾—ä¸æ»¡è¶³äºä½œè€…æä¾›çš„ç¤ºä¾‹å›¾ç‰‡ã€‚

![](img/684573c5ed0bbf77b24aac6095f07919.png)

æˆ‘åœ¨æ–‡æ¡£æ–¹é¢æœ‰ç‚¹çº ç»“ã€‚[æ¶æ„æ–‡æ¡£](https://kengz.gitbooks.io/slm-lab/content/usage/aeb-design.html)æ˜¯æœ‰é™çš„ï¼Œå…¶ä½™çš„å…³æ³¨äºä½¿ç”¨ã€‚ä½†æ˜¯æˆ‘è¯´çš„ä½¿ç”¨æ˜¯æŒ‡[åœ¨å½“å‰çš„å®ç°ä¸Šè¿è¡Œå®éªŒ](https://kengz.gitbooks.io/slm-lab/content/usage/spec-file.html)ã€‚æˆ‘åŠªåŠ›å¯»æ‰¾å‘Šè¯‰æˆ‘å¦‚ä½•ä»¥ä¸åŒæ–¹å¼å°†æ¨¡å—è¿æ¥åœ¨ä¸€èµ·çš„æ–‡æ¡£ã€‚æˆ‘æ¨æµ‹ä»–ä»¬æ‰“ç®—é€šè¿‡ JSON è§„èŒƒæ–‡ä»¶æ¥å®Œæˆè¿™é¡¹å·¥ä½œã€‚äº‹å®ä¸Šï¼Œæœ€åˆçš„åŠ¨æœºæ˜¯:

> æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ¡†æ¶ï¼Œå…è®¸æˆ‘ä»¬æ¯”è¾ƒç®—æ³•å’Œç¯å¢ƒï¼Œå¿«é€Ÿå»ºç«‹å®éªŒæ¥æµ‹è¯•å‡è®¾ï¼Œé‡ç”¨ç»„ä»¶ï¼Œåˆ†æå’Œæ¯”è¾ƒç»“æœï¼Œè®°å½•ç»“æœã€‚

æ‰€ä»¥è¿™é‡Œçš„ç›®æ ‡æ˜¯å…è®¸é€šè¿‡é…ç½®é‡ç”¨ï¼Œå¾ˆåƒ[å¤šå·´èƒº](#google-dopamine-https-github-com-google-dopamine)å’Œ [Tensorforce](#tensorforce-https-github-com-tensorforce-tensorforce) ã€‚è¿™ä¸ªâ€œRL ä½œä¸ºé…ç½®â€å¥½åƒæ˜¯ä¸€ä¸ªä¸»é¢˜ï¼ç„¶è€Œï¼Œæˆ‘å¹¶ä¸ä¿¡æœã€‚æˆ‘è®¤ä¸ºä»£ç æ›´ä¹ æƒ¯ï¼Œæ›´çµæ´»ã€‚è¿™æ˜¯äººä»¬æ‰€ä¹ æƒ¯çš„ã€‚æ¯æ¬¡ä½ é€šè¿‡é…ç½®åšä¸€äº›äº‹æƒ…ï¼Œè¿™æ˜¯ç”¨æˆ·å¿…é¡»å­¦ä¹ çš„å¦ä¸€ç§*é¢†åŸŸç‰¹å®šè¯­è¨€* (DSL)ã€‚å› ä¸º DSL é€šå¸¸æ˜¯é™æ€(Gin ä¸æ˜¯)ï¼Œæ‰€ä»¥ DSL çš„å®ç°è®¾ç½®äº†é™åˆ¶ã€‚å®ƒæ°¸è¿œä¸ä¼šé€‚åˆæ¯ä¸ªäººï¼Œå› ä¸ºä¼šæœ‰ DSL æ²¡æœ‰è¦†ç›–çš„è¾¹ç¼˜æƒ…å†µã€‚

### [é¹¿](https://github.com/VinF/deer)

![](img/9c3ef7fcd1ba1231f8967924f28a6405.png)![](img/741d8aea5a7992af22cc2e8e4a5e6af9.png)![](img/bd2810ce6fc7e2f22173a1805235ccbe.png)![](img/10d644990a18d63f69bde5374bf47624.png)

é¹¿æœ€åˆçš„å°è±¡æ˜¯å¥½çš„ã€‚å®ƒæœ‰ä¸€ä¸ª pip å®‰è£…ç¨‹åºã€‚å®ƒæ¸²æŸ“äº†æ¨¡å—åŒ–çš„æŠ’æƒ…ã€‚å®ƒåªæ‹¥æœ‰**ä¸¤æ¡**å·¨èŸ’çš„ä¾èµ–ï¼›`numpy`å’Œ`Joblib`ã€‚æ‰€ä»¥æ²¡æœ‰è®¨åŒçš„ C make è¿‡ç¨‹æ¥è®©å®ƒå·¥ä½œï¼Œå¤ªå¥½äº†ï¼

æ–‡æ¡£æ˜¯æ¸…æ™°çš„ï¼Œä½†æ˜¯ç¼ºå°‘ä¸€äº›æ•´ä½“æ¶æ„æ–‡æ¡£ã€‚ä½ å¿…é¡»é’»ç ”ç±»/ä»£ç æ¥æ‰¾åˆ°æ–‡æ¡£ã€‚ä½†æ˜¯å½“ä½ è¿™æ ·åšçš„æ—¶å€™æ˜¯å¥½çš„ã€‚

â€œæ¨¡å—â€ä¸»è¦æ˜¯æŒ‰ç…§æ‚¨æ‰€æœŸæœ›çš„æ–¹å¼è¿›è¡Œåˆ’åˆ†çš„ã€‚æ¨¡å—ä¸º[`Environment`](https://deer.readthedocs.io/en/master/modules/environments.html)[`Agent`](https://deer.readthedocs.io/en/master/modules/agents.html)[`Policies`](https://deer.readthedocs.io/en/master/modules/policies.html)ã€‚

æœ‰ä¸ªæœ‰è¶£çš„ç±»å« [`Controller`](https://deer.readthedocs.io/en/master/modules/controllers.html) ä¸æ ‡å‡†ã€‚è¯¥ç±»æä¾›äº†å¯ä»¥é™„åŠ çš„ç”Ÿå‘½å‘¨æœŸæŒ‚é’©ï¼›äº‹ä»¶ï¼Œå¦‚ä¸€é›†çš„ç»“å°¾æˆ–ä»»ä½•æ—¶å€™é‡‡å–è¡ŒåŠ¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³åœ¨ä¸€é›†çš„ç»“å°¾åšä¸€äº›æ—¥å¿—è®°å½•ï¼Œé‚£ä¹ˆä½ å¯ä»¥å­ç±»åŒ–è¿™ä¸ªç±»å¹¶è¦†ç›– [`onEpisodeEnd`](https://github.com/VinF/deer/blob/master/deer/experiment/base_controllers.py#L52) ã€‚æ§åˆ¶å™¨æœ‰å‡ ä¸ªä¾‹å­ï¼Œå…¶ä¸­ä¸€ä¸ªæ˜¯ [`EpsilonController`](https://github.com/VinF/deer/blob/master/deer/experiment/base_controllers.py#L149) ã€‚è¿™å…è®¸ä½ åŠ¨æ€æ”¹å˜è´ªå©ªç®—æ³•ä¸­çš„`eta`æˆ–`epsilon`å€¼ã€‚

è¿™æ˜¯éå¸¸å¼ºå¤§çš„ï¼Œå› ä¸ºå®ƒå…è®¸ä½ ä¸­é€”æ”¹å˜å­¦ä¹ è¿‡ç¨‹ã€‚ä½†æ˜¯ä»è½¯ä»¶å·¥ç¨‹çš„è§’åº¦æ¥çœ‹ï¼Œè¿™æ˜¯éå¸¸å±é™©çš„ã€‚ä»»ä½•å‡½æ•°å¼ç¨‹åºå‘˜éƒ½ä¼šå‘Šè¯‰ä½ ä¸è¦æ”¹å˜å¦ä¸€ä¸ªå¯¹è±¡çš„çŠ¶æ€ï¼Œå› ä¸ºâ€œé¾™åœ¨è¿™é‡Œâ€ã€‚å¦‚æœ API æ›´å…·åŠŸèƒ½æ€§ï¼Œå¹¶ä¸”å¯ä»¥ä¼ é€’è®¡ç®—ä»£ç†çš„ next `eta`çš„å‡½æ•°ï¼Œè€Œä¸æ˜¯ç›´æ¥æ”¹å˜ä»£ç†çš„çŠ¶æ€ï¼Œé‚£å°±æ›´å¥½äº†ã€‚ä¸è¿‡ï¼Œè¿™å¯èƒ½ä¼šè®©äº‹æƒ…ç¨å¾®å¤æ‚ä¸€ç‚¹ã€‚

è¿™ä¸ªæ¡†æ¶ä¹ŸåŒ…å«äº†ä¸€äº›å­¦ä¹ ç®—æ³•ï¼Œä½†æ˜¯å®ƒè‚¯å®šæ²¡æœ‰åƒ T2 è”»é©° T3 é‚£æ ·å…¨é¢ã€‚

#### å…¥é—¨æŒ‡å—

ç”±äº pip å®‰è£…å’Œå¾ˆå°‘çš„ä¾èµ–ï¼Œè¿™å¯èƒ½æ˜¯æœ€å®¹æ˜“å¯åŠ¨å’Œè¿è¡Œçš„æ¡†æ¶ã€‚

```py
!pip install git+git://github.com/VINF/deer.git@master
!git clone https://github.com/VinF/deer.git 
```

æˆ‘å…‹éš†äº† git repoï¼Œè¿™æ ·æˆ‘å°±å¯ä»¥è¿è¡Œç¤ºä¾‹äº†ã€‚æ¥ä¸‹æ¥å°±æ˜¯å¯¼å…¥æ‰€æœ‰å†…å®¹çš„é—®é¢˜äº†:

```py
%cd /content/deer/examples/toy_env
import numpy as np
from deer.agent import NeuralAgent
from deer.learning_algos.q_net_keras import MyQNetwork
from Toy_env import MyEnv as Toy_env
import deer.experiment.base_controllers as bc 
```

å·æ¢ä¾‹å­:

```py
rng = np.random.RandomState(123456)

# --- Instantiate environment ---
env = Toy_env(rng)

# --- Instantiate qnetwork ---
qnetwork = MyQNetwork(
    environment=env,
    random_state=rng)

# --- Instantiate agent ---
agent = NeuralAgent(
    env,
    qnetwork,
    random_state=rng)

# --- Bind controllers to the agent ---
# Before every training epoch, we want to print a summary of the agent's epsilon, discount and
# learning rate as well as the training epoch number.
agent.attach(bc.VerboseController())

# During training epochs, we want to train the agent after every action it takes.
# Plus, we also want to display after each training episode (!= than after every training) the average bellman
# residual and the average of the V values obtained during the last episode.
agent.attach(bc.TrainerController())

# All previous controllers control the agent during the epochs it goes through. However, we want to interleave a
# "test epoch" between each training epoch. We do not want these test epoch to interfere with the training of the
# agent. Therefore, we will disable these controllers for the whole duration of the test epochs interleaved this
# way, using the controllersToDisable argument of the InterleavedTestEpochController. The value of this argument
# is a list of the indexes of all controllers to disable, their index reflecting in which order they were added.
agent.attach(bc.InterleavedTestEpochController(
    epoch_length=500,
    controllers_to_disable=[0, 1]))

# --- Run the experiment ---
agent.run(n_epochs=100, epoch_length=1000) 
```

è¿™é‡Œæˆ‘ä»¬å®ä¾‹åŒ–äº†ä¸€ä¸ªç¯å¢ƒï¼Œåˆ›å»ºäº† Q-Learning ç®—æ³•ï¼Œå¹¶åˆ›å»ºäº†ä½¿ç”¨è¯¥ç®—æ³•çš„ä»£ç†ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»£ç†ä¸Šçš„`.attach()`å‡½æ•°æ¥è°ƒç”¨æˆ‘ä»¬ä¸€ç›´åœ¨è®¨è®ºçš„æ‰€æœ‰è¿™äº›`Controller`ã€‚ä»–ä»¬å¢åŠ äº†æ—¥å¿—è®°å½•ï¼Œå¹¶äº¤é”™äº†åŸ¹è®­æœŸå’Œæµ‹è¯•æœŸã€‚

å¦‚æœæˆ‘ä»¬æƒ³ç¼–è¾‘è¿™äº›ï¼Œæˆ‘ä»¬åªéœ€è¦é‡æ–°å®ç°æˆ‘ä»¬æ„Ÿå…´è¶£çš„éƒ¨åˆ†ã€‚å¤ªå¥½äº†ï¼

![](img/e0e37067f640154d14a0e2fad56a2d36.png)

å”¯ä¸€çš„é—®é¢˜æ˜¯ç©å…·çš„ä¾‹å­ä¸å·¥ä½œï¼ğŸ¤¦æˆ‘ä¸çŸ¥é“ä¸ºä»€ä¹ˆï¼Œä½†è¿™å¯èƒ½æ˜¯ä¸€äº›æ„šè ¢çš„äº‹æƒ…ã€‚è®­ç»ƒå€¼çœ‹èµ·æ¥æœ‰ç‚¹å¥‡æ€ªï¼Œå› ä¸ºæµ‹è¯•åˆ†æ•°æ€»æ˜¯ 0ï¼Œè®­ç»ƒæŸå¤±éšç€æ—¶é—´çš„æ¨ç§»è€Œå¢åŠ ã€‚å¯èƒ½å¡ä½äº†ã€‚ä¸ç¡®å®šã€‚æˆ‘æ•¢è‚¯å®šè¿™æ˜¯ä¸€äº›æ„šè ¢çš„äº‹æƒ…ã€‚

### [è½¦åº“](https://github.com/rlworkgroup/garage)

![](img/638e1413658def3c7ea9d9cc4ac7e849.png)![](img/0422c477d35525ebdcf844de3ed52065.png)![](img/0fada9983862ad3dda4673b9e258165a.png)![](img/72c93a56fc9ef6b3f7668932263fe425.png)

Garage æ˜¯ rllab çš„åç»­äº§å“ï¼Œç›®æ ‡ç›¸åŒï¼Œä½†åªæ˜¯ç¤¾åŒºï¼Œè€Œä¸æ˜¯ä¸ªäººæ”¯æŒã€‚[æ–‡æ¡£](https://rlgarage.readthedocs.io/en/latest/index.html)æœ‰ç‚¹ç¨€ç–ã€‚ä¾‹å¦‚ï¼Œå®ƒæ²¡æœ‰å¼ºè°ƒå®ƒå®ç°äº†[å¤§é‡çš„ç®—æ³•](https://github.com/rlworkgroup/garage/tree/master/src/garage/tf/algos)ã€‚è¿˜æœ‰[æ•°é‡åºå¤§çš„ä¿å•](https://github.com/rlworkgroup/garage/tree/master/src/garage/tf/policies)ã€‚äº‹å®ä¸Šï¼Œåœ¨è¿™ä¸ª[å®‰é™çš„å°ç›®å½•](https://github.com/rlworkgroup/garage/tree/master/src/garage/tf)ä¸­æœ‰ä½ å¯èƒ½éœ€è¦çš„å‡ ä¹æ‰€æœ‰ä¸œè¥¿ã€‚

ä½†æ˜¯å®ƒä¸å¼ é‡æµç´§å¯†è€¦åˆï¼Œå¦‚æœè¿™å¯¹ä½ æœ‰é—®é¢˜çš„è¯ã€‚

è¿™é‡Œæœ‰å¦‚æ­¤å¤šçš„åŠŸèƒ½ï¼Œä½†å®ƒæ˜¯å®Œå…¨éšè—çš„ã€‚ä»£ç è¢«åˆç†åœ°å¾ˆå¥½åœ°è®°å½•ï¼Œä½†æ˜¯å®ƒæ²¡æœ‰è¢«å…¬å¼€ã€‚ä½ å¿…é¡»æŒ–æ˜å®ƒæ‰èƒ½æ‰¾åˆ°å®ƒã€‚

åŒæ ·æ²¡æœ‰ pip å®‰è£…ç¨‹åºã€‚åªæ˜¯ä¸€äº›å®šåˆ¶çš„ conda å®‰è£…å’Œä¸€äº›`apt-get`ä¾èµ–ã€‚

å› æ­¤ï¼Œæˆ‘å¯ä»¥çœ‹åˆ°ç®—æ³•å®ç°æœ‰å·¨å¤§çš„ä»·å€¼ï¼Œä½†è¿™æ¬¡æˆ‘å°†è·³è¿‡å…¥é—¨éƒ¨åˆ†ã€‚

### [è¶…ç°å®](https://github.com/SurrealAI/surreal)

![](img/2c40f027c0898e9fa0dd541877205622.png)![](img/e016ac009ddc90f2b0b70944167ccde7.png)![](img/bece8bf71e1ce3aa2c0ab9f7a31a6076.png)![](img/445f9657822b39fd1bcbf42b42b2b63c.png)

è¶…ç°å®æ˜¯ä¸€å¥—åº”ç”¨ç¨‹åºã€‚é¦–å…ˆï¼Œå®ƒæ˜¯ä¸€ä¸ª RL æ¡†æ¶ã€‚ä½†ä¸ºäº†ç¡®ä¿ä»–ä»¬ä¸åªæ˜¯å»ºç«‹å¦ä¸€ä¸ªæ¡†æ¶ï¼Œä»–ä»¬è¿˜æä¾›äº†ä¸€ä¸ªæ–°çš„æœºå™¨äººæ¨¡æ‹Ÿå™¨ï¼Œä¸€ä¸ªåè°ƒå™¨ï¼Œä¸€ä¸ªäº‘åŸºç¡€è®¾æ–½ä¾›åº”å™¨å’Œä¸€ä¸ªåˆ†å¸ƒå¼è®¡ç®—åè®®ã€‚å®ƒæ¥è‡ªæ–¯å¦ç¦ï¼Œå› æ­¤åœ¨æ–¹æ³•å’Œç”¨ä¾‹ä¸Šæ˜¯å­¦æœ¯æ€§çš„ï¼Œå› æ­¤é»˜è®¤ä½¿ç”¨ PyTorchã€‚

æˆ‘å®Œå…¨æ”¯æŒæ¡†æ¶å’Œæ¨¡æ‹Ÿå™¨ï¼Œä½†å¦‚æœä»–ä»¬åªæ˜¯ä½¿ç”¨ orchestrator (Kubernetes)ã€infrastructure (Terraform)å’Œ protocol (Kafka/Nats/etc/etc)çš„æ ‡å‡†å·¥ä¸šç»„ä»¶ï¼Œäº‹æƒ…ä¼šæ›´å®¹æ˜“ã€‚è¿™äº›é—®é¢˜å·²ç»è§£å†³äº†ã€‚(æ›´æ­£å“ˆå“ˆã€‚å½“æˆ‘æ·±å…¥äº†è§£å…¥é—¨æŒ‡å—æ—¶ï¼Œæˆ‘å‘ç°ä»–ä»¬æ­£åœ¨ä½¿ç”¨ Kubernetes å’Œ Terraformã€‚ä¼Ÿå¤§çš„é€‰æ‹©ï¼ğŸ˜‚)

æœºå™¨äººæ¨¡æ‹Ÿå™¨æ˜¯ä¸€ç»„æ¨¡æ‹Ÿæœºå™¨äººçš„é›†åˆã€‚æ‰€ä»¥è¿™æ˜¯å¯¹ç¯å¢ƒåˆ—è¡¨çš„ä¸€ä¸ªå¾ˆå¥½çš„è¡¥å……(å°½ç®¡æœ‰ MuJoCo çš„è®¸å¯æ¡æ¬¾)ã€‚

RL æ¡†æ¶éœ€è¦å¤§é‡çš„å“„éª—ã€‚è¿™æ˜¯ apt-get å’Œ conda å®‰è£…çš„å¦ä¸€ç§ç»„åˆã€‚

å“¦å“‡ã€‚æˆ‘åˆšåˆšæ³¨æ„åˆ°ä»–ä»¬ç¦ç”¨äº† Github é—®é¢˜è·Ÿè¸ªå™¨ã€‚å¹¶ä¸”æœ‰ä¸€ä¸ªæ˜ç¡®çš„ç‰ˆæƒå£°æ˜å±äºæ¯ä¸ªä½œè€…ã€‚å¥½å§ï¼Œè¿™ç”šè‡³éƒ½ä¸æ˜¯å¼€æºçš„ã€‚

ä½†æ˜¯æœºå™¨äººæ˜¯éº»çœç†å·¥è®¸å¯çš„ï¼Ÿ

å¾ˆå¥‡æ€ªã€‚ç”±äºç¼ºå°‘é—®é¢˜å’Œ werid è®¸å¯ï¼Œæ­¤å¤„åœæ­¢ã€‚

### [RLgraph](https://github.com/rlgraph/rlgraph)

![](img/68da0b982a3d9e839a8d454f2b25794b.png)![](img/c50dc20de248c63ffca111972da0ebcf.png)![](img/87e539de061303ef177091f76e383696.png)![](img/da8cb3f45e9ad405ad6ff44467070d5f.png)

æ‰€ä»¥è®©æˆ‘ä»¬å…ˆè¯´ RLgraph æœ‰å¤§é‡çš„æäº¤ã€‚ä»–ä»¬ä»¥æ¯å¹´ 4000 æ¬¡æäº¤çš„é€Ÿåº¦è¿è¡Œã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒOpenAI å¥èº«æˆ¿åªæœ‰ 221 è‹±é•‘ã€‚æœ‰äººéœ€è¦å‘Šè¯‰è¿™äº”ä¸ªäººå»åº¦å‡ã€‚è€Œä¸”æ‰ä¸€å²ã€‚æˆ‘åªèƒ½æƒ³è±¡å®ƒè¢«å…¨èŒä½¿ç”¨ã€‚

ä½†æ— è®ºå¦‚ä½•ï¼Ÿåƒå…¶ä»–æ¡†æ¶ä¸€æ ·ï¼Œä»–ä»¬å…³æ³¨å¯ä¼¸ç¼©æ€§ã€‚ä½†æœ‰è¶£çš„æ˜¯ï¼Œå®ƒä»¬ç›´æ¥æ˜ å°„åˆ° Tensorflow å’Œ Pytorchã€‚ä»–ä»¬æ²¡æœ‰ä½¿ç”¨ Kerasã€‚æ‰€ä»¥è¿™æœ¬èº«è‚¯å®šæ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚çœ‹èµ·æ¥ä»–ä»¬ä½¿ç”¨äº†[å°„çº¿é¡¹ç›®](#rllib-https-ray-readthedocs-io-en-latest-rllib-html-via-ray-project-https-github-com-ray-project-ray)æ¥åˆ†é…å·¥ä½œï¼Œå°±åƒ [SLM-Lab](http://localhost:1313/a-comparison-of-reinforcement-learning-frameworks/#slm-lab-https-github-com-kengz-slm-lab) ä¸€æ ·ã€‚

ä½†æ˜¯ä¸‡å²ï¼ä»–ä»¬æœ‰ä¸€ä¸ª pip å®‰è£…ç¨‹åºã€‚ä»£ç†çš„é…ç½®æ˜¯é€šè¿‡ JSON æ§åˆ¶çš„ã€‚ä½†ä»…ä»…æ˜¯é…ç½®ã€‚ä¸æ˜¯å»ºç­‘ã€‚

æˆ‘åˆšåˆšè¯»åˆ°ä½œè€…[ä¹Ÿåœ¨ Tensorforce](https://rlgraph.github.io/rlgraph/2019/01/04/introducing-rlgraph.html) ä¸Šå·¥ä½œï¼Œè¿™è§£é‡Šäº†æˆ‘ä¸€ç›´æ„Ÿè§‰åˆ°çš„ä¸€äº› de ja vouxã€‚æˆ‘å–œæ¬¢æˆ‘åœ¨ Tensorforce ä¸­çš„æŠ±æ€¨ï¼Œå…³äºåº•å±‚ DL æ¡†æ¶å¦‚ä½•ç»å¸¸æ³„æ¼åˆ° RL å®ç°ä»£ç ä¸­ï¼Œå·²ç»åœ¨ RLgraph ä¸­å¾—åˆ°è§£å†³ã€‚æˆ‘è§‰å¾—ä»–ä»¬ä¸€ç›´åœ¨å¬æˆ‘å¯¹æˆ‘æ— èŠçš„å¦»å­å’†å“®ã€‚

> ä»é€»è¾‘ç»„åˆä¸­åˆ†ç¦»å¼ é‡ç©ºé—´ä½¿æˆ‘ä»¬èƒ½å¤Ÿé‡ç”¨ç»„ä»¶ï¼Œè€Œä¸å¿…å†æ¬¡æ‰‹åŠ¨å¤„ç†ä¸å…¼å®¹çš„å½¢çŠ¶ã€‚è¯·æ³¨æ„ï¼Œä¸Šé¢çš„ä»£ç ä¸åŒ…å«ä»»ä½•ç‰¹å®šäºæ¡†æ¶çš„æ¦‚å¿µï¼Œè€Œåªå®šä¹‰äº†ä¸€ç»„ç©ºé—´çš„è¾“å…¥æ•°æ®æµã€‚

åªæ˜¯æƒ³è¦æˆ‘ä¸€ç›´æƒ³è¦çš„ã€‚è¿™æ˜¯é€šè¿‡[è¾“å…¥å’Œè¾“å‡ºçš„æŠ½è±¡](https://rlgraph.readthedocs.io/en/latest/spaces.html)å®ç°çš„ã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒAPI æ˜¯ç†Ÿæ‚‰çš„ã€‚ä¸€ä¸ª [`Environment`](https://rlgraph.readthedocs.io/en/latest/environments.html) å’Œä¸€ä¸ª [`Agent`](https://github.com/rlgraph/rlgraph/blob/master/rlgraph/agents/agent.py#L42) ã€‚æœ‰ä¸€ä¸ªéå¸¸é…·çš„ [`Component`](https://rlgraph.readthedocs.io/en/latest/components.html) ç±»ï¼Œå®ƒæŠ½è±¡äº† DL æ„ä»¶ã€‚

ç„¶è€Œï¼Œè¿™é‡Œç¼ºå°‘ä¸€äº›æŠ½è±¡ã€‚æ²¡æœ‰æ”¿ç­–æŠ½è±¡ã€‚æ²¡æœ‰æ¢ç´¢æŠ½è±¡ã€‚åŸºæœ¬ä¸Šæ‰€æœ‰å¥½çš„æŠ½è±¡éƒ½æ¥è‡ªè”»é©°çš„ Nervana ç³»ç»Ÿå…¬å¸ã€‚

ä½†æˆ‘è¿˜æ˜¯å¾ˆæ„ŸåŠ¨ã€‚

#### å…¥é—¨æŒ‡å—

æˆ‘ç¨å¾®ä¿®æ”¹äº†ä¸€ä¸‹ cartpole å…¥é—¨ç¤ºä¾‹ï¼Œä½¿ç”¨ SingleThreadedWorker å¹¶åœ¨ç¯å¢ƒä¸Šå¯ç”¨æ¸²æŸ“ä»¥è·å¾—è§†é¢‘è¾“å‡ºã€‚é™¤æ­¤ä¹‹å¤–ï¼Œä¸€åˆ‡çœ‹èµ·æ¥éƒ½å¾ˆç†Ÿæ‚‰ã€‚

```py
import numpy as np
from rlgraph.agents import DQNAgent
from rlgraph.environments import OpenAIGymEnv
from rlgraph.execution import SingleThreadedWorker

environment = OpenAIGymEnv('CartPole-v0', monitor=".", monitor_video=1, visualize=True)

# Create from .json file or dict, see agent API for all
# possible configuration parameters.
agent = DQNAgent.from_file(
  "configs/dqn_cartpole.json",
  state_space=environment.state_space,
  action_space=environment.action_space
)

episode_returns = []

def episode_finished_callback(episode_return, duration, timesteps, **kwargs):
  episode_returns.append(episode_return)
  if len(episode_returns) % 10 == 0:
    print("Episode {} finished: reward={:.2f}, average reward={:.2f}.".format(
      len(episode_returns), episode_return, np.mean(episode_returns[-10:])
    ))

worker = SingleThreadedWorker(env_spec=lambda: environment, agent=agent, render=True, worker_executes_preprocessing=False,
                              episode_finish_callback=episode_finished_callback)
print("Starting workload, this will take some time for the agents to build.")

# Use exploration is true for training, false for evaluation.
worker.execute_timesteps(1000, use_exploration=True) 
```

[https://winder.ai/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/rlgraph.mp4](https://winder.ai/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/rlgraph.mp4)

### [ç®€å• RL](https://github.com/david-abel/simple_rl)

![](img/5e1569f6846f80691df8f23070bc8a8c.png)![](img/241d3fbc6e5fa62d6881817900b7c66f.png)![](img/babe2f42d22f884d1b03ea8810e57e0a.png)![](img/8ca8bce5011d086d7998aa7cb0019bff.png)

æœ€åï¼Œç®€å• _rlã€‚æ‰€æœ‰å…¶ä»–æ¡†æ¶éƒ½å£°æ˜å®ƒä»¬çš„ç›®æ ‡æ˜¯æ€§èƒ½/å¯ä¼¸ç¼©æ€§æˆ–æ¨¡å—åŒ–æˆ–å¯é‡å¤æ€§ã€‚æ²¡æœ‰ä¸€ä¸ªæ˜¯ç®€å•çš„ã€‚è¿™å°±æ˜¯ simple_rl ä»‹å…¥çš„åœ°æ–¹ã€‚ä»å¤´å¼€å§‹å»ºé€ ï¼Œå°½å¯èƒ½ç®€å•ã€‚å®ƒåªæœ‰ä¸¤ä¸ªä¾èµ–é¡¹ï¼Œ`numpy`å’Œ`matplotlib`ã€‚è€Œä¸”è¿™åªæ˜¯åœ¨ä½ æƒ³ç”»å‡ºç»“æœçš„æ—¶å€™ã€‚åŸºæœ¬ä¸Šå®ƒåªæ˜¯`numpy`ã€‚å®ƒæœ‰ [pip å®‰è£…ç¨‹åº](https://github.com/david-abel/simple_rl#installation)ã€‚[æ–‡æ¡£æ˜¯ä¸å­˜åœ¨çš„](https://david-abel.github.io/simple_rl/docs/index.html)ä½†æ˜¯æ²¡å…³ç³»ï¼Œè°éœ€è¦æ–‡æ¡£ï¼Ÿï¼›-)

å®ƒå‘ˆç°äº†ä¸€ä¸ªç†Ÿæ‚‰çš„æŠ½è±¡é›†åˆ:ä¸€ä¸ª [`agent`](https://github.com/david-abel/simple_rl/blob/master/simple_rl/agents/AgentClass.py) ï¼Œä¸€ä¸ª [`experiment`](https://github.com/david-abel/simple_rl/blob/master/simple_rl/experiments/ExperimentClass.py) ï¼Œä¸€ä¸ªè¢«ç§°ä¸º [`mdp`](https://github.com/david-abel/simple_rl/blob/master/simple_rl/mdp/MDPClass.py) çš„ç¯å¢ƒã€‚è¯¥æ¡†æ¶è¿˜æŠ½è±¡äº†æ¨¡å‹çš„å…¶ä»–éƒ¨åˆ†ï¼Œå¦‚åŠ¨ä½œã€ç‰¹å¾ã€çŠ¶æ€ã€‚ä»¥åŠå®ç°ä¸‹ä¸€æ­¥è¡ŒåŠ¨ç­–ç•¥çš„è§„åˆ’ç±»ã€‚å®ƒä»ç„¶æ˜¯éå¸¸æ¨¡å—åŒ–çš„ï¼Œä½†æ˜¯ä¸€äº›å‘½åçº¦å®šåº”è¯¥è¢«æ”¹å˜ä»¥åŒ¹é…å…¶ä»–æ¡†æ¶(æ ‡å‡†åŒ–)ã€‚

æ‰€ä»¥å¾ˆæ˜æ˜¾ï¼Œâ€œç®€å•â€å¹¶ä¸ä¸€å®šæ„å‘³ç€å®¹æ˜“ç†è§£ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œè¶ŠæŠ½è±¡è¶Šéš¾ç†è§£ã€‚ç®€å•æ¥è¯´å°±æ˜¯â€œæ˜“ç”¨æ€§â€ã€‚æˆ‘è®¤ä¸ºé‚£æ˜¯ä¸€ç§è€»è¾±ã€‚æˆ‘çœŸçš„å¸Œæœ›åœ¨ç†è§£æ–¹é¢ç®€å•ä¸€äº›ã€‚ä½†çœ‹èµ·æ¥å®ƒçš„ç›®æ ‡æ˜¯ä¸ä¸€äº›æ›´å¤æ‚çš„æ¡†æ¶ç«äº‰ï¼›ã€PyTorch çš„æ·±åº¦ RL æ”¯æŒæ­£åœ¨å¼€å‘ä¸­ã€‚

å¯¹äºéå¸¸ç®€å•ã€å¯ç†è§£çš„ RL æ¡†æ¶ï¼Œæ¡†æ¶å¸‚åœºä»ç„¶å­˜åœ¨ç¼ºå£ã€‚æˆ‘ä¹Ÿä¸ç¡®å®šä¸ºä»€ä¹ˆè¿™ä¸ªæ¡†æ¶å’Œå…¶ä»–æ¡†æ¶ç›¸æ¯”æ˜Ÿæ˜Ÿè¿™ä¹ˆå°‘ã€‚å¤§æ¦‚æ˜¯å› ä¸ºå®ƒä¸åƒè®¸å¤šå…¶ä»–æ¡†æ¶é‚£æ ·ä¾èµ–äºå…¶ä»– DL æ¡†æ¶çš„æµè¡Œã€‚

#### å…¥é—¨æŒ‡å—

æœ¬æ¥åº”è¯¥å¾ˆç®€å•ã€‚ä½†æ˜¯åœ¨ä»£ç æ·±å¤„ï¼Œæœ‰å‡ è¡Œä»£ç å¼ºåˆ¶`Matplotlib`ä½¿ç”¨`TkAgg`åç«¯ã€‚æˆ‘è¯•å›¾è®©`TkAgg`åœ¨ç¬”è®°æœ¬ä¸Šå·¥ä½œï¼Œä½†æ˜¯åšä¸åˆ°ã€‚å®ƒæ˜¯ä¸ºå›¾å½¢æ¡Œé¢ä½¿ç”¨è€Œè®¾è®¡çš„ï¼Œæ‰€ä»¥ä½ å¯ä»¥æƒ³è±¡å®ƒå¹¶ä¸ç®€å•ã€‚æˆ‘åœ¨è¿™é‡Œåˆ¶é€ äº†ä¸€ä¸ª[é—®é¢˜](https://github.com/david-abel/simple_rl/issues/40)ã€‚è¿™åº”è¯¥æ˜¯ä¸€ä¸ªç®€å•çš„ä¿®å¤ã€‚

å¦‚æœ/å½“å®ƒèµ·ä½œç”¨æ—¶ï¼Œåº”è¯¥åƒä¸‹é¢è¿™æ ·ç®€å•:

```py
from simple_rl.agents import QLearningAgent, RandomAgent, RMaxAgent
from simple_rl.tasks import GridWorldMDP
from simple_rl.run_experiments import run_agents_on_mdp

# Setup MDP.
mdp = GridWorldMDP(width=4, height=3, init_loc=(1, 1), goal_locs=[(4, 3)], lava_locs=[(4, 2)], gamma=0.95, walls=[(2, 2)], slip_prob=0.05)

# Setup Agents.
ql_agent = QLearningAgent(actions=mdp.get_actions())
rmax_agent = RMaxAgent(actions=mdp.get_actions())
rand_agent = RandomAgent(actions=mdp.get_actions())

# Run experiment and make plot.
run_agents_on_mdp([ql_agent, rmax_agent, rand_agent], mdp, instances=5, episodes=50, steps=10) 
```

è¿™è®­ç»ƒäº†å‡ ä¸ªä¸åŒçš„ä»£ç†ï¼Œå¹¶ä¸ºæ¯ä¸ªä»£ç†åˆ¶ä½œäº†ä¸€ä¸ªå¥–åŠ±å›¾ã€‚ä¸é”™å§ã€‚æˆ‘å”¯ä¸€å»ºè®®çš„æ˜¯ï¼Œåœ¨ simple_rl ä¸­ä¸åº”è¯¥æœ‰ä»»ä½•ç¯å¢ƒå®ç°ã€‚è¿™è¶…å‡ºäº†èŒƒå›´ã€‚æŠŠå®ƒç•™ç»™ç±»ä¼¼å¥èº«æˆ¿çš„é¡¹ç›®å§ã€‚ä¾‹å¦‚ï¼Œ [gym-minigrid](https://github.com/maximecb/gym-minigrid) æœ‰ä¸€ä¸ªä»¤äººæ•¬ç•çš„ Gridworld å®ç°ã€‚

*   : 652

## è°·æ­Œæ’å

è°·æ­Œçš„è¶‹åŠ¿æœç´¢å·¥å…·å¯ä»¥è®©ä½ æ‰¾å‡ºä»€ä¹ˆæœç´¢æŸ¥è¯¢æ˜¯æœ€å—æ¬¢è¿çš„ã€‚ä¸å¹¸çš„æ˜¯ï¼Œå®ƒä»¬åªæä¾›ç›¸å¯¹çš„åº¦é‡ï¼Œå¹¶ä¸”è¿™äº›åº¦é‡ä¼šæ ¹æ®æ‚¨æŸ¥è¯¢çš„å†…å®¹è€Œå˜åŒ–ã€‚æ­¤å¤–ï¼Œå¸¸ç”¨è¯ç»å¸¸ä¼šæ··å…¥å…¶ä»–æŸ¥è¯¢ä¸­ã€‚æ¯”å¦‚æœç´¢â€œè„¸ä¹¦åœ°å¹³çº¿â€ï¼Œå¤¹æ‚ç€ä¸€å †å…³äºâ€œForza Horizon 4â€â€œFacebook ç™»å½•â€çš„ä¸ç›¸å…³æŸ¥è¯¢ï¼›æ˜¾ç„¶ï¼Œè¿™å¤¸å¤§äº†åˆ†æ•°ï¼Œä¸å¯ä¿¡ã€‚

æˆ‘æµè§ˆäº†æ‰€æœ‰è¿™äº›æ¡†æ¶ï¼Œå‘ç°åªæœ‰ä¸¤ä¸ªæ¡†æ¶è„±é¢–è€Œå‡ºï¼Œopenai gym å’Œ google dopamineã€‚ä½†å³ä½¿æ˜¯è°·æ­Œå¤šå·´èƒºï¼Œç›¸å…³çš„æŸ¥è¯¢ä¹Ÿæ˜¯è°·æ­Œæ–‡æ¡£/å­¦è€…/ç¿»è¯‘ç­‰ã€‚ï¼Œæ‰€ä»¥æˆ‘ä¹Ÿä¸ç¡®å®šè¿™ä¸ªèƒ½ä¸èƒ½ä¿¡ã€‚

å¯¹æˆ‘æ¥è¯´æœ€çªå‡ºçš„ä¸€ç‚¹æ˜¯åœ°ç†ä¸Šçš„å—æ¬¢è¿ç¨‹åº¦ã€‚OpenAI Gym ä¼¼ä¹æ˜¯æœ€å—æ¬¢è¿çš„æœç´¢è¯ï¼Œå› ä¸ºå®ƒæœ‰å¾ˆé«˜çš„æ’ååˆ†æ•°ï¼Œå¹¶ä¸”æ‰€æœ‰ç›¸å…³æŸ¥è¯¢éƒ½ä¸ RL ç›¸å…³ã€‚ä½†æ˜¯å½“ä½ çœ‹åˆ°æ’åå¦‚ä½•å› åœ°ç†ä½ç½®è€Œå˜åŒ–æ—¶ï¼Œä¸­å›½æ˜¯æœç´¢æ¬¡æ•°æœ€å¤šçš„å›½å®¶ã€‚

è¿™è®©æˆ‘è§‰å¾—å¾ˆå¥‡æ€ªï¼Œå› ä¸ºè°·æ­Œåœ¨ä¸­å›½è¢«ç¦ï¼Œé‚£ä¹ˆä»–ä»¬æ˜¯å¦‚ä½•äº§ç”Ÿè¿™äº›æ•°æ®çš„å‘¢ï¼Ÿç”¨æˆ·ä½¿ç”¨è™šæ‹Ÿä¸“ç”¨ç½‘ç„¶åæœç´¢ï¼Œè°·æ­Œèƒ½å¤Ÿè¯†åˆ«åŸå§‹æµé‡æ¥è‡ªä¸­å›½å—ï¼Ÿ

### ä¸è¦ç›¸ä¿¡è°·æ­Œè¶‹åŠ¿

æ‰€æœ‰è¿™äº›éƒ½è®©æˆ‘å¾—å‡ºä¸€ä¸ªç»“è®ºï¼Œé‚£å°±æ˜¯æˆ‘æ ¹æœ¬ä¸èƒ½ç›¸ä¿¡è°·æ­Œè¶‹åŠ¿ã€‚OpenAI Gym ç¡®å®çœ‹èµ·æ¥åƒæ˜¯æ’åæœ€é«˜çš„ RL ç›¸å…³æ¡†æ¶ï¼Œè¿™å¯èƒ½æ˜¯ä½ æ‰€æœŸæœ›çš„ï¼Œä½†å¤§éƒ¨åˆ†åˆ†æ•°æ¥è‡ªä¸­å›½ã€‚ä½†æ˜¯è°·æ­Œåœ¨ä¸­å›½è¢«å±è”½äº†ã€‚Sooo &mldr;..ğŸ¤·