# 602:最近邻分类和回归

> 原文:[https://winder . ai/602-最近邻分类回归/](https://winder.ai/602-nearest-neighbour-classification-and-regression/)

## 不仅仅是相似

*   分类:预测与最近的观察值相同的类别
*   回归:预测与最接近的观察值相同的值

？？？

记住，对于分类任务，我们希望预测一个新观察的类别。

我们能做的是预测与最近邻相同的类。简单！

对于回归任务，我们需要预测一个值。同样，我们可以使用最近邻的值！又简单了！

* * *

## 最近邻分类

![Nearest neighbour classification](../Images/f2876d1f076d2f93ab60caa6c7583397.png)

* * *

## 最近邻回归

![Nearest neighbour regression](../Images/4d9aa59a71fafc67ff68981982012c26.png)

？？？

我们在这些例子中使用了最近邻来演示这个想法。

但实际上，我们不想只使用一个邻居。可能是噪音。

相反，我们可以使用一些邻居的加权。例如在分类中，我们将预测具有大多数最近邻的类。对于回归，我们可以取平均值。

同样，我们可以通过查看属于预测类别的最近邻的比例来估计类别概率。

(小心少量观察的类概率！)

通常，当我们使用一个以上的邻居时，该算法的名称被缩短为 *k-NN* 。

其中 k 是指算法中使用的邻居数量。

一般来说，k 值越高，平均效果越好，结果越“平滑”。

让我们再来看看 k 值不同的“iris”数据集。

* * *

![iris_knn](../Images/2882b50bc0308da970368e32b365ff20.png)

* * *

## 偏差和方差部分 duex

我们在这里看到的是实践中的过度拟合和过度概括。

通过选择低 k 值，我们对异常值高度敏感。我们吃多了。

但是选择较高的 k 值，模型可能不够复杂，不足以表示数据。

* * *

### 我们如何选择 k 的值？

我们之前看到了如何使用验证来挑选，以确保我们没有过度或不足。

我们可以在这里做同样的事情，改变 k 的值并验证结果。

但是要注意 k 的选择，应该是&mldr;

* * *

*   类别数和 K 的互质

例如，两个类，如果我们使用 6 的 K 值，我们可以有联系。

*   大于或等于类别数加 1

给每个班级发言的机会。

*   高到足以避免虚假结果
*   低到足以避免总是选择最常见的类

* * *

## 赞成/反对

使用 k-NN 的好处是显而易见的，并且在开始时就已经说明了。

*   这个算法很容易理解。
*   它非常灵活；你可以用它来进行相似性匹配、分类或回归。
*   很容易调；通常只有一个简单的参数。

然而，还有一些问题。我们接下来会讨论。

* * *

### 正当理由；辩解

*   你如何证明这个结果是正确的？

例如

> 你的抵押贷款申请被拒绝了，因为你与三个违约者相似，他们也是丹麦人

*   我们创建了一个复杂的查找表。它并没有提高我们对数据的了解。

？？？

首先，这不一定只适用于 k-NN，你如何解释和证明这个结果？

网飞为他们的 k-NN 推荐辩护说:“你可能喜欢《我是艾伦·帕特里奇》，因为你喜欢《黄铜眼》和《波拉特》。你可能会接受这个理由。

但如果你被拒绝抵押贷款，因为“你的申请被拒绝，因为你的情况(你居住的地方)与违约者相似”。这可能不太好嚼。

此外，从模型中学到一些东西更难，因为我们实际上没有建模任何东西。我们基本上创建了一个复杂的查找表。利益相关者可能不喜欢这个结果。

* * *