# 基于强化学习的库存控制和供应链优化

> 原文：<https://winder.ai/inventory-control-and-supply-chain-optimization-with-reinforcement-learning/>

[库存控制](https://en.wikipedia.org/wiki/Inventory_control)是在给定业务的独特约束和要求的情况下，试图优化产品或库存水平的问题。这是一个重要的问题，因为每个以商品为基础的企业都必须花费资源来维持库存水平，以便他们能够提供客户想要的产品。库存控制的每一项改进都会直接改善业务的交付。新手研究战术，专家研究后勤，所以说。

[强化学习](https://winder.ai/services/#reinforcement-learning) (RL)是机器学习的一个子学科，它为一个以全球业务为中心的目标优化重复、连续的决策。RL 经常出现在机器人、定价和推荐等挑战中，但它特别适合自动化、优化的库存控制和供应链管理。

## 库存控制中的常见问题

即使是中等规模的零售企业也经常库存大约 100，000 种不同的产品，这些产品可能分布在一个很大的地理区域内，可能由 1，000 家实体店组成。仓库或配送中心可以作为产品的临时仓库。这种业务的目标是将货物运送到最需要的地方。但购买取决于需求、可获得性和复杂的全球环境的混合。

即使是不直接向消费者销售的企业也存在复杂的链条，其中可能包括运输、分销、销售商和供应商。任何这些都可能导致上游(您的供应商)和下游(您的经销商)的问题，结果是销售的损失。

### 复杂分层路由

仓库和商店通过各种运输机制重新进货，包括公路、铁路、海运或空运。在这个过程中有多种权衡，从根本上取决于当前的库存水平、物品损耗的速度、需求和产能。供应链试图管理库存水平，以防止过度库存(这会增加成本和浪费)和库存不足(会限制销售)。当这些中的任何一个不是最佳时，就会出现问题。

### 供应链不协调

即使在单一组织的有限范围内，库存管理也是一项艰巨的挑战。但许多业务往往依赖于其他第三方公司的库存。缺乏协调会导致[“牛鞭”效应](https://www.cips.org/knowledge/procurement-topics-and-skills/operations-management/bullwhip-effect-in-supply-chain/)，当零售商需求的变化通过供应链传播回来时就会出现这种情况。

## 用强化学习解决库存控制问题

在小范围内，[混合整数线性规划](https://en.wikipedia.org/wiki/Integer_programming)可用于模拟企业内的联合库存需求和约束，但这仅限于少量产品(通常少于 10 个)和短时间范围。实际实现必须使用退化试探法或需求假设。

像[模型预测控制](https://en.wikipedia.org/wiki/Model_predictive_control)和[动态规划](https://en.wikipedia.org/wiki/Dynamic_programming)这样的数据驱动方法是潜在的解决方案，因为它们朝着最佳库存水平迭代。然而，这些方法要么需要完全了解任何时候的股票走势(也就是说，你需要知道客户什么时候会购买)，要么至少需要知道产品的转移概率。在大多数情况下，这两种方法都不可用，最多只能依靠试探法。

[强化学习](https://rl-book.com) (RL)是机器学习(ML)的一个子学科，它优化涉及多个连续决策的问题，以达到最佳战略或政策。换句话说，RL 主动学习模型来描述整个零售商、仓库和供应商网络中的产品移动，并在给定一组约束和目标的情况下得出最佳库存策略。RL 被认为优于以前的方法，因为它假设更少，学习更多，这使它更普遍适用。

### 将商店/仓库建模为 RL 代理

一种潜在的 RL 建模方法是将每个商店和仓库想象成一个不同的 RL 代理，就像 [Sultana et al.](https://arxiv.org/abs/2006.04037) 中那样，然后每个商店代理反复与仓库互动，根据当前库存水平、预测销售额、估计补货延迟和预测损耗来补充库存。仓库以相似的输入和输出运作，但与名义上的供应商互动。奖励可以独立设计，也可以联合设计，以最大化预期目标，比如一个恒定的水平。

 

### 多智能体，协作式 RL 智能体

对先前想法的一个扩展是使用多代理、协作 RL，其中所有代理对所有其他代理具有完全的可观测性。这种方法的好处是代理可以利用包含在其他代理中的信息。

例如，前一种方法的一个问题是，商店需要时间来学习最佳策略。如果发生了戏剧性的事件，比如新冠肺炎事件，商店需要时间来重新培训他们的政策以适应新环境。在此期间，仓库试图学习次优化行为。

一个简单的解决方案是减缓或延迟仓库的学习，直到商店行为稳定下来。但这是令人遗憾的(从这个词的技术意义上来说)。

如果可行，另一种方法是允许仓库查看商店的状态。这种完美的信息应该减少培训时间。

### 多主体，合作，不完善知识

在许多供应链中，你看不到另一方的库存，但供应链从合作中受益。例如，最大化供应链的效率符合种植者的最大利益，这样他们就可以卖出更多的产品。这对零售商来说也是一个福音，因为他们不想库存过多或过少。

在这种情况下，可以将每个实体视为一个代理，不仅针对当地条件(如库存过多或不足)进行优化，还针对全球可观察的指标(如销售的产品总数)进行优化，类似于 [Oroojlooyjadid 等人](https://arxiv.org/abs/1708.05924)中所述。

这种方法的主要好处是它减轻了“牛鞭”效应，因为供应链的每一级都有一个具有代表性的上下链模型。

 

## 工业应用

十年来，像 Zara 这样的全球零售商一直在使用 ML 来优化他们的库存控制问题。最近，像[美国航空](https://arxiv.org/abs/1902.06824)这样的组织已经开始寻求 RL 来提供优化座位超额预订的解决方案。

RL 正迅速成为首选工具，因为它能够解决复杂的库存控制问题，这在以前是不可行的。

最近的工作是提供更好的模拟器，结合更有效的算法，如 T2 离线学习或使用指导的算法，这意味着进入的门槛比以往任何时候都低。

### 如何开发 RL 驱动的解决方案

你可能会发现自己认为设计、开发和部署一个 RL 驱动的解决方案是令人畏惧的。的确，RL 解决方案很难，依赖于 ML、软件工程和 RL 方面的专业知识，但在帮助下是可以实现的。我们写了一本关于在商业中使用[强化学习](https://rl-book.com)的书，我们在工作中体验了这一点。

理论上，RL 和其他软件工程项目没有什么不同。这样计划和安排工作是可能的。但是类似于一个数据科学项目，你需要保留一些时间来处理风险和失败。过于复杂和定义不明确的目标总是第一个问题，但是对项目简单性或数据可用性的假设也会导致重大问题。

具体来说，在 RL 中，我们发现需要投入更多的时间来构建模拟，以消除作战部署的风险。并且需要大量的设计时间来规划不会对业务产生负面影响的部署。正因为如此，RL 项目往往是较长时期的较大投资。这意味着 RL 最适合于高影响问题。

我们在我们的书中写了更多，但最简单快捷的解决方案是[给我们打电话](https://winder.ai/about/contact/)。